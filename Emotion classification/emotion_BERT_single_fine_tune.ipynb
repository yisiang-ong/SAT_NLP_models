{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import logging\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "from argparse import Namespace\n",
    "from packaging import version\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is partly adapted from the link below:\n",
    "# https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"emotion_data/my_train.txt\"\n",
    "test_path = \"emotion_data/my_test.txt\"\n",
    "val_path = \"emotion_data/my_val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary associating each string label to an integer value\n",
    "\n",
    "labels = [ \"sadness\", \"joy\", \"anger\", \"fear\"]\n",
    "label2int = dict(zip(labels, list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# We are going to use bert-base-uncased\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i gotta say for the first time in a long while...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am pissed</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im kind of firm as the school year is coming t...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today im really content</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im not fine my team was disqualified and i los...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    class\n",
       "0  i gotta say for the first time in a long while...      joy\n",
       "1                                        i am pissed    anger\n",
       "2  im kind of firm as the school year is coming t...  sadness\n",
       "3                            today im really content      joy\n",
       "4  im not fine my team was disqualified and i los...    anger"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"emotion_data/my_train.txt\",sep=\";\", header=None)\n",
    "data1 = pd.read_csv(\"emotion_data/my_val.txt\",sep=\";\", header=None)\n",
    "data2 = pd.read_csv(\"emotion_data/my_test.txt\",sep=\";\", header=None)\n",
    "data.columns = [\"text\",\"class\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my ex ruined my life and i am mad\n"
     ]
    }
   ],
   "source": [
    "samele_row = data.iloc[5]\n",
    "sample_text = samele_row.text\n",
    "# sample_text = \"I have a few exercises in mind that you could really benefit from and bursh up on {a dichotomy} you may be a little hesitant and this is okay, you can trust me and yourself!\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "encoding.keys()\n",
    "encoding[\"input_ids\"].shape, encoding[\"attention_mask\"].shape\n",
    "# print(encoding[\"input_ids\"].cpu().detach().numpy())\n",
    "# tokenizer.decode(encoding[\"input_ids\"].cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'my', 'ex', 'ruined', 'my', 'life', 'and', 'i', 'am', 'mad', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "encoding[\"input_ids\"].squeeze()[:20]\n",
    "print(tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "token_counts = []\n",
    "for _, row in data.iterrows():\n",
    "    token_count = len(tokenizer.encode(\n",
    "        row[\"text\"],\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ))\n",
    "    token_counts.append(token_count)\n",
    "sns.histplot(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_COUNT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoDataset(Dataset):\n",
    "  def __init__(\n",
    "    self,\n",
    "    path,\n",
    "    tokenizer: BertTokenizer,\n",
    "    max_token_len: int = 100\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data_column = \"text\"\n",
    "    self.class_column = \"class\"\n",
    "    self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                            engine=\"python\")\n",
    "    \n",
    "    self.max_token_len = max_token_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "    text = data_row.text\n",
    "    labels = label2int[data_row[\"class\"]]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return (encoding[\"input_ids\"].flatten(),encoding[\"attention_mask\"].flatten()), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 101, 2026, 4654, 9868, 2026, 2166, 1998, 1045, 2572, 5506,  102,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0])),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EmoDataset(\n",
    "    train_path,\n",
    "    tokenizer,\n",
    "    max_token_len=100\n",
    ")\n",
    "sample_item = train_dataset[5]\n",
    "sample_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[1]\n",
    "# sample_item[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 100]), torch.Size([16, 100]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(DataLoader(train_dataset, batch_size=16)))\n",
    "sample_batch[0][0].shape, sample_batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert_model(sample_batch[0][0], sample_batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 100, 768]), torch.Size([16, 768]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 768 dimension comes from the BERT hidden size\n",
    "output.last_hidden_state.shape, output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom classifcation model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Mish activation function \n",
    "#(from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py)\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "  \n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an EmoClassificationModel class to do the actual fine-tuning\n",
    "\n",
    "class EmoClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "        \n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, *args):\n",
    "\n",
    "        hidden_states = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch Lightning for training.\n",
    "#we use PyTorch Lighning for training. Lightning methods are defined here\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmoClassificationModel(BertModel.from_pretrained(\"bert-base-uncased\"), len(labels)) #was \"distilroberta-base\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.max_token_length = 128\n",
    "        self.loss = nn.CrossEntropyLoss() #cross entropy loss since this is multi-class classification\n",
    "        # self.save_hyperparameters(hparams)\n",
    "        self.hparams = hparams\n",
    "        self.loss_amount = 0.\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        loss = self.loss(self.forward(input_ids=X[0], attention_mask = X[1]), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        \n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmoDataset(ds_path, self.tokenizer, self.max_token_length),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr) #we use AdamW as this usually performs well\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "   \n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), 'emotion_model/BERT_emotion_1ft.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=20,\n",
    "    warmup_steps=100,\n",
    "    epochs=30,\n",
    "    lr=2E-06,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)\n",
    "#rubbish collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # monitor validation loss\n",
    "    min_delta=0.001, #to very small change in the monitored quantity to qualify as an improvement\n",
    "    patience=20, # used to check number of time with no improvement after which training will be stopped\n",
    "    verbose=False, \n",
    "    mode=\"min\" #sed while training will stopped when the quantity monitor has stopped decreasing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:lightning:\n",
      "    | Name                                                              | Type                   | Params\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "0   | model                                                             | EmoClassificationModel | 110 M \n",
      "1   | model.base_model                                                  | BertModel              | 109 M \n",
      "2   | model.base_model.embeddings                                       | BertEmbeddings         | 23 M  \n",
      "3   | model.base_model.embeddings.word_embeddings                       | Embedding              | 23 M  \n",
      "4   | model.base_model.embeddings.position_embeddings                   | Embedding              | 393 K \n",
      "5   | model.base_model.embeddings.token_type_embeddings                 | Embedding              | 1 K   \n",
      "6   | model.base_model.embeddings.LayerNorm                             | LayerNorm              | 1 K   \n",
      "7   | model.base_model.embeddings.dropout                               | Dropout                | 0     \n",
      "8   | model.base_model.encoder                                          | BertEncoder            | 85 M  \n",
      "9   | model.base_model.encoder.layer                                    | ModuleList             | 85 M  \n",
      "10  | model.base_model.encoder.layer.0                                  | BertLayer              | 7 M   \n",
      "11  | model.base_model.encoder.layer.0.attention                        | BertAttention          | 2 M   \n",
      "12  | model.base_model.encoder.layer.0.attention.self                   | BertSelfAttention      | 1 M   \n",
      "13  | model.base_model.encoder.layer.0.attention.self.query             | Linear                 | 590 K \n",
      "14  | model.base_model.encoder.layer.0.attention.self.key               | Linear                 | 590 K \n",
      "15  | model.base_model.encoder.layer.0.attention.self.value             | Linear                 | 590 K \n",
      "16  | model.base_model.encoder.layer.0.attention.self.dropout           | Dropout                | 0     \n",
      "17  | model.base_model.encoder.layer.0.attention.output                 | BertSelfOutput         | 592 K \n",
      "18  | model.base_model.encoder.layer.0.attention.output.dense           | Linear                 | 590 K \n",
      "19  | model.base_model.encoder.layer.0.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "20  | model.base_model.encoder.layer.0.attention.output.dropout         | Dropout                | 0     \n",
      "21  | model.base_model.encoder.layer.0.intermediate                     | BertIntermediate       | 2 M   \n",
      "22  | model.base_model.encoder.layer.0.intermediate.dense               | Linear                 | 2 M   \n",
      "23  | model.base_model.encoder.layer.0.intermediate.intermediate_act_fn | GELUActivation         | 0     \n",
      "24  | model.base_model.encoder.layer.0.output                           | BertOutput             | 2 M   \n",
      "25  | model.base_model.encoder.layer.0.output.dense                     | Linear                 | 2 M   \n",
      "26  | model.base_model.encoder.layer.0.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "27  | model.base_model.encoder.layer.0.output.dropout                   | Dropout                | 0     \n",
      "28  | model.base_model.encoder.layer.1                                  | BertLayer              | 7 M   \n",
      "29  | model.base_model.encoder.layer.1.attention                        | BertAttention          | 2 M   \n",
      "30  | model.base_model.encoder.layer.1.attention.self                   | BertSelfAttention      | 1 M   \n",
      "31  | model.base_model.encoder.layer.1.attention.self.query             | Linear                 | 590 K \n",
      "32  | model.base_model.encoder.layer.1.attention.self.key               | Linear                 | 590 K \n",
      "33  | model.base_model.encoder.layer.1.attention.self.value             | Linear                 | 590 K \n",
      "34  | model.base_model.encoder.layer.1.attention.self.dropout           | Dropout                | 0     \n",
      "35  | model.base_model.encoder.layer.1.attention.output                 | BertSelfOutput         | 592 K \n",
      "36  | model.base_model.encoder.layer.1.attention.output.dense           | Linear                 | 590 K \n",
      "37  | model.base_model.encoder.layer.1.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "38  | model.base_model.encoder.layer.1.attention.output.dropout         | Dropout                | 0     \n",
      "39  | model.base_model.encoder.layer.1.intermediate                     | BertIntermediate       | 2 M   \n",
      "40  | model.base_model.encoder.layer.1.intermediate.dense               | Linear                 | 2 M   \n",
      "41  | model.base_model.encoder.layer.1.output                           | BertOutput             | 2 M   \n",
      "42  | model.base_model.encoder.layer.1.output.dense                     | Linear                 | 2 M   \n",
      "43  | model.base_model.encoder.layer.1.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "44  | model.base_model.encoder.layer.1.output.dropout                   | Dropout                | 0     \n",
      "45  | model.base_model.encoder.layer.2                                  | BertLayer              | 7 M   \n",
      "46  | model.base_model.encoder.layer.2.attention                        | BertAttention          | 2 M   \n",
      "47  | model.base_model.encoder.layer.2.attention.self                   | BertSelfAttention      | 1 M   \n",
      "48  | model.base_model.encoder.layer.2.attention.self.query             | Linear                 | 590 K \n",
      "49  | model.base_model.encoder.layer.2.attention.self.key               | Linear                 | 590 K \n",
      "50  | model.base_model.encoder.layer.2.attention.self.value             | Linear                 | 590 K \n",
      "51  | model.base_model.encoder.layer.2.attention.self.dropout           | Dropout                | 0     \n",
      "52  | model.base_model.encoder.layer.2.attention.output                 | BertSelfOutput         | 592 K \n",
      "53  | model.base_model.encoder.layer.2.attention.output.dense           | Linear                 | 590 K \n",
      "54  | model.base_model.encoder.layer.2.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "55  | model.base_model.encoder.layer.2.attention.output.dropout         | Dropout                | 0     \n",
      "56  | model.base_model.encoder.layer.2.intermediate                     | BertIntermediate       | 2 M   \n",
      "57  | model.base_model.encoder.layer.2.intermediate.dense               | Linear                 | 2 M   \n",
      "58  | model.base_model.encoder.layer.2.output                           | BertOutput             | 2 M   \n",
      "59  | model.base_model.encoder.layer.2.output.dense                     | Linear                 | 2 M   \n",
      "60  | model.base_model.encoder.layer.2.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "61  | model.base_model.encoder.layer.2.output.dropout                   | Dropout                | 0     \n",
      "62  | model.base_model.encoder.layer.3                                  | BertLayer              | 7 M   \n",
      "63  | model.base_model.encoder.layer.3.attention                        | BertAttention          | 2 M   \n",
      "64  | model.base_model.encoder.layer.3.attention.self                   | BertSelfAttention      | 1 M   \n",
      "65  | model.base_model.encoder.layer.3.attention.self.query             | Linear                 | 590 K \n",
      "66  | model.base_model.encoder.layer.3.attention.self.key               | Linear                 | 590 K \n",
      "67  | model.base_model.encoder.layer.3.attention.self.value             | Linear                 | 590 K \n",
      "68  | model.base_model.encoder.layer.3.attention.self.dropout           | Dropout                | 0     \n",
      "69  | model.base_model.encoder.layer.3.attention.output                 | BertSelfOutput         | 592 K \n",
      "70  | model.base_model.encoder.layer.3.attention.output.dense           | Linear                 | 590 K \n",
      "71  | model.base_model.encoder.layer.3.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "72  | model.base_model.encoder.layer.3.attention.output.dropout         | Dropout                | 0     \n",
      "73  | model.base_model.encoder.layer.3.intermediate                     | BertIntermediate       | 2 M   \n",
      "74  | model.base_model.encoder.layer.3.intermediate.dense               | Linear                 | 2 M   \n",
      "75  | model.base_model.encoder.layer.3.output                           | BertOutput             | 2 M   \n",
      "76  | model.base_model.encoder.layer.3.output.dense                     | Linear                 | 2 M   \n",
      "77  | model.base_model.encoder.layer.3.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "78  | model.base_model.encoder.layer.3.output.dropout                   | Dropout                | 0     \n",
      "79  | model.base_model.encoder.layer.4                                  | BertLayer              | 7 M   \n",
      "80  | model.base_model.encoder.layer.4.attention                        | BertAttention          | 2 M   \n",
      "81  | model.base_model.encoder.layer.4.attention.self                   | BertSelfAttention      | 1 M   \n",
      "82  | model.base_model.encoder.layer.4.attention.self.query             | Linear                 | 590 K \n",
      "83  | model.base_model.encoder.layer.4.attention.self.key               | Linear                 | 590 K \n",
      "84  | model.base_model.encoder.layer.4.attention.self.value             | Linear                 | 590 K \n",
      "85  | model.base_model.encoder.layer.4.attention.self.dropout           | Dropout                | 0     \n",
      "86  | model.base_model.encoder.layer.4.attention.output                 | BertSelfOutput         | 592 K \n",
      "87  | model.base_model.encoder.layer.4.attention.output.dense           | Linear                 | 590 K \n",
      "88  | model.base_model.encoder.layer.4.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "89  | model.base_model.encoder.layer.4.attention.output.dropout         | Dropout                | 0     \n",
      "90  | model.base_model.encoder.layer.4.intermediate                     | BertIntermediate       | 2 M   \n",
      "91  | model.base_model.encoder.layer.4.intermediate.dense               | Linear                 | 2 M   \n",
      "92  | model.base_model.encoder.layer.4.output                           | BertOutput             | 2 M   \n",
      "93  | model.base_model.encoder.layer.4.output.dense                     | Linear                 | 2 M   \n",
      "94  | model.base_model.encoder.layer.4.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "95  | model.base_model.encoder.layer.4.output.dropout                   | Dropout                | 0     \n",
      "96  | model.base_model.encoder.layer.5                                  | BertLayer              | 7 M   \n",
      "97  | model.base_model.encoder.layer.5.attention                        | BertAttention          | 2 M   \n",
      "98  | model.base_model.encoder.layer.5.attention.self                   | BertSelfAttention      | 1 M   \n",
      "99  | model.base_model.encoder.layer.5.attention.self.query             | Linear                 | 590 K \n",
      "100 | model.base_model.encoder.layer.5.attention.self.key               | Linear                 | 590 K \n",
      "101 | model.base_model.encoder.layer.5.attention.self.value             | Linear                 | 590 K \n",
      "102 | model.base_model.encoder.layer.5.attention.self.dropout           | Dropout                | 0     \n",
      "103 | model.base_model.encoder.layer.5.attention.output                 | BertSelfOutput         | 592 K \n",
      "104 | model.base_model.encoder.layer.5.attention.output.dense           | Linear                 | 590 K \n",
      "105 | model.base_model.encoder.layer.5.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "106 | model.base_model.encoder.layer.5.attention.output.dropout         | Dropout                | 0     \n",
      "107 | model.base_model.encoder.layer.5.intermediate                     | BertIntermediate       | 2 M   \n",
      "108 | model.base_model.encoder.layer.5.intermediate.dense               | Linear                 | 2 M   \n",
      "109 | model.base_model.encoder.layer.5.output                           | BertOutput             | 2 M   \n",
      "110 | model.base_model.encoder.layer.5.output.dense                     | Linear                 | 2 M   \n",
      "111 | model.base_model.encoder.layer.5.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "112 | model.base_model.encoder.layer.5.output.dropout                   | Dropout                | 0     \n",
      "113 | model.base_model.encoder.layer.6                                  | BertLayer              | 7 M   \n",
      "114 | model.base_model.encoder.layer.6.attention                        | BertAttention          | 2 M   \n",
      "115 | model.base_model.encoder.layer.6.attention.self                   | BertSelfAttention      | 1 M   \n",
      "116 | model.base_model.encoder.layer.6.attention.self.query             | Linear                 | 590 K \n",
      "117 | model.base_model.encoder.layer.6.attention.self.key               | Linear                 | 590 K \n",
      "118 | model.base_model.encoder.layer.6.attention.self.value             | Linear                 | 590 K \n",
      "119 | model.base_model.encoder.layer.6.attention.self.dropout           | Dropout                | 0     \n",
      "120 | model.base_model.encoder.layer.6.attention.output                 | BertSelfOutput         | 592 K \n",
      "121 | model.base_model.encoder.layer.6.attention.output.dense           | Linear                 | 590 K \n",
      "122 | model.base_model.encoder.layer.6.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "123 | model.base_model.encoder.layer.6.attention.output.dropout         | Dropout                | 0     \n",
      "124 | model.base_model.encoder.layer.6.intermediate                     | BertIntermediate       | 2 M   \n",
      "125 | model.base_model.encoder.layer.6.intermediate.dense               | Linear                 | 2 M   \n",
      "126 | model.base_model.encoder.layer.6.output                           | BertOutput             | 2 M   \n",
      "127 | model.base_model.encoder.layer.6.output.dense                     | Linear                 | 2 M   \n",
      "128 | model.base_model.encoder.layer.6.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "129 | model.base_model.encoder.layer.6.output.dropout                   | Dropout                | 0     \n",
      "130 | model.base_model.encoder.layer.7                                  | BertLayer              | 7 M   \n",
      "131 | model.base_model.encoder.layer.7.attention                        | BertAttention          | 2 M   \n",
      "132 | model.base_model.encoder.layer.7.attention.self                   | BertSelfAttention      | 1 M   \n",
      "133 | model.base_model.encoder.layer.7.attention.self.query             | Linear                 | 590 K \n",
      "134 | model.base_model.encoder.layer.7.attention.self.key               | Linear                 | 590 K \n",
      "135 | model.base_model.encoder.layer.7.attention.self.value             | Linear                 | 590 K \n",
      "136 | model.base_model.encoder.layer.7.attention.self.dropout           | Dropout                | 0     \n",
      "137 | model.base_model.encoder.layer.7.attention.output                 | BertSelfOutput         | 592 K \n",
      "138 | model.base_model.encoder.layer.7.attention.output.dense           | Linear                 | 590 K \n",
      "139 | model.base_model.encoder.layer.7.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "140 | model.base_model.encoder.layer.7.attention.output.dropout         | Dropout                | 0     \n",
      "141 | model.base_model.encoder.layer.7.intermediate                     | BertIntermediate       | 2 M   \n",
      "142 | model.base_model.encoder.layer.7.intermediate.dense               | Linear                 | 2 M   \n",
      "143 | model.base_model.encoder.layer.7.output                           | BertOutput             | 2 M   \n",
      "144 | model.base_model.encoder.layer.7.output.dense                     | Linear                 | 2 M   \n",
      "145 | model.base_model.encoder.layer.7.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "146 | model.base_model.encoder.layer.7.output.dropout                   | Dropout                | 0     \n",
      "147 | model.base_model.encoder.layer.8                                  | BertLayer              | 7 M   \n",
      "148 | model.base_model.encoder.layer.8.attention                        | BertAttention          | 2 M   \n",
      "149 | model.base_model.encoder.layer.8.attention.self                   | BertSelfAttention      | 1 M   \n",
      "150 | model.base_model.encoder.layer.8.attention.self.query             | Linear                 | 590 K \n",
      "151 | model.base_model.encoder.layer.8.attention.self.key               | Linear                 | 590 K \n",
      "152 | model.base_model.encoder.layer.8.attention.self.value             | Linear                 | 590 K \n",
      "153 | model.base_model.encoder.layer.8.attention.self.dropout           | Dropout                | 0     \n",
      "154 | model.base_model.encoder.layer.8.attention.output                 | BertSelfOutput         | 592 K \n",
      "155 | model.base_model.encoder.layer.8.attention.output.dense           | Linear                 | 590 K \n",
      "156 | model.base_model.encoder.layer.8.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "157 | model.base_model.encoder.layer.8.attention.output.dropout         | Dropout                | 0     \n",
      "158 | model.base_model.encoder.layer.8.intermediate                     | BertIntermediate       | 2 M   \n",
      "159 | model.base_model.encoder.layer.8.intermediate.dense               | Linear                 | 2 M   \n",
      "160 | model.base_model.encoder.layer.8.output                           | BertOutput             | 2 M   \n",
      "161 | model.base_model.encoder.layer.8.output.dense                     | Linear                 | 2 M   \n",
      "162 | model.base_model.encoder.layer.8.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "163 | model.base_model.encoder.layer.8.output.dropout                   | Dropout                | 0     \n",
      "164 | model.base_model.encoder.layer.9                                  | BertLayer              | 7 M   \n",
      "165 | model.base_model.encoder.layer.9.attention                        | BertAttention          | 2 M   \n",
      "166 | model.base_model.encoder.layer.9.attention.self                   | BertSelfAttention      | 1 M   \n",
      "167 | model.base_model.encoder.layer.9.attention.self.query             | Linear                 | 590 K \n",
      "168 | model.base_model.encoder.layer.9.attention.self.key               | Linear                 | 590 K \n",
      "169 | model.base_model.encoder.layer.9.attention.self.value             | Linear                 | 590 K \n",
      "170 | model.base_model.encoder.layer.9.attention.self.dropout           | Dropout                | 0     \n",
      "171 | model.base_model.encoder.layer.9.attention.output                 | BertSelfOutput         | 592 K \n",
      "172 | model.base_model.encoder.layer.9.attention.output.dense           | Linear                 | 590 K \n",
      "173 | model.base_model.encoder.layer.9.attention.output.LayerNorm       | LayerNorm              | 1 K   \n",
      "174 | model.base_model.encoder.layer.9.attention.output.dropout         | Dropout                | 0     \n",
      "175 | model.base_model.encoder.layer.9.intermediate                     | BertIntermediate       | 2 M   \n",
      "176 | model.base_model.encoder.layer.9.intermediate.dense               | Linear                 | 2 M   \n",
      "177 | model.base_model.encoder.layer.9.output                           | BertOutput             | 2 M   \n",
      "178 | model.base_model.encoder.layer.9.output.dense                     | Linear                 | 2 M   \n",
      "179 | model.base_model.encoder.layer.9.output.LayerNorm                 | LayerNorm              | 1 K   \n",
      "180 | model.base_model.encoder.layer.9.output.dropout                   | Dropout                | 0     \n",
      "181 | model.base_model.encoder.layer.10                                 | BertLayer              | 7 M   \n",
      "182 | model.base_model.encoder.layer.10.attention                       | BertAttention          | 2 M   \n",
      "183 | model.base_model.encoder.layer.10.attention.self                  | BertSelfAttention      | 1 M   \n",
      "184 | model.base_model.encoder.layer.10.attention.self.query            | Linear                 | 590 K \n",
      "185 | model.base_model.encoder.layer.10.attention.self.key              | Linear                 | 590 K \n",
      "186 | model.base_model.encoder.layer.10.attention.self.value            | Linear                 | 590 K \n",
      "187 | model.base_model.encoder.layer.10.attention.self.dropout          | Dropout                | 0     \n",
      "188 | model.base_model.encoder.layer.10.attention.output                | BertSelfOutput         | 592 K \n",
      "189 | model.base_model.encoder.layer.10.attention.output.dense          | Linear                 | 590 K \n",
      "190 | model.base_model.encoder.layer.10.attention.output.LayerNorm      | LayerNorm              | 1 K   \n",
      "191 | model.base_model.encoder.layer.10.attention.output.dropout        | Dropout                | 0     \n",
      "192 | model.base_model.encoder.layer.10.intermediate                    | BertIntermediate       | 2 M   \n",
      "193 | model.base_model.encoder.layer.10.intermediate.dense              | Linear                 | 2 M   \n",
      "194 | model.base_model.encoder.layer.10.output                          | BertOutput             | 2 M   \n",
      "195 | model.base_model.encoder.layer.10.output.dense                    | Linear                 | 2 M   \n",
      "196 | model.base_model.encoder.layer.10.output.LayerNorm                | LayerNorm              | 1 K   \n",
      "197 | model.base_model.encoder.layer.10.output.dropout                  | Dropout                | 0     \n",
      "198 | model.base_model.encoder.layer.11                                 | BertLayer              | 7 M   \n",
      "199 | model.base_model.encoder.layer.11.attention                       | BertAttention          | 2 M   \n",
      "200 | model.base_model.encoder.layer.11.attention.self                  | BertSelfAttention      | 1 M   \n",
      "201 | model.base_model.encoder.layer.11.attention.self.query            | Linear                 | 590 K \n",
      "202 | model.base_model.encoder.layer.11.attention.self.key              | Linear                 | 590 K \n",
      "203 | model.base_model.encoder.layer.11.attention.self.value            | Linear                 | 590 K \n",
      "204 | model.base_model.encoder.layer.11.attention.self.dropout          | Dropout                | 0     \n",
      "205 | model.base_model.encoder.layer.11.attention.output                | BertSelfOutput         | 592 K \n",
      "206 | model.base_model.encoder.layer.11.attention.output.dense          | Linear                 | 590 K \n",
      "207 | model.base_model.encoder.layer.11.attention.output.LayerNorm      | LayerNorm              | 1 K   \n",
      "208 | model.base_model.encoder.layer.11.attention.output.dropout        | Dropout                | 0     \n",
      "209 | model.base_model.encoder.layer.11.intermediate                    | BertIntermediate       | 2 M   \n",
      "210 | model.base_model.encoder.layer.11.intermediate.dense              | Linear                 | 2 M   \n",
      "211 | model.base_model.encoder.layer.11.output                          | BertOutput             | 2 M   \n",
      "212 | model.base_model.encoder.layer.11.output.dense                    | Linear                 | 2 M   \n",
      "213 | model.base_model.encoder.layer.11.output.LayerNorm                | LayerNorm              | 1 K   \n",
      "214 | model.base_model.encoder.layer.11.output.dropout                  | Dropout                | 0     \n",
      "215 | model.base_model.pooler                                           | BertPooler             | 590 K \n",
      "216 | model.base_model.pooler.dense                                     | Linear                 | 590 K \n",
      "217 | model.base_model.pooler.activation                                | Tanh                   | 0     \n",
      "218 | model.classifier                                                  | Sequential             | 593 K \n",
      "219 | model.classifier.0                                                | Dropout                | 0     \n",
      "220 | model.classifier.1                                                | Linear                 | 590 K \n",
      "221 | model.classifier.2                                                | Mish                   | 0     \n",
      "222 | model.classifier.3                                                | Dropout                | 0     \n",
      "223 | model.classifier.4                                                | Linear                 | 3 K   \n",
      "224 | loss                                                              | CrossEntropyLoss       | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:  74%|  | 40/54 [00:10<00:03,  3.98it/s, loss=0.105, train_loss=0.104, v_num=134] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train (using cuda)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "                     early_stop_callback=early_stop_callback)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "________________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness     0.8148    0.8462    0.8302        26\n",
      "         joy     0.9706    1.0000    0.9851        33\n",
      "       anger     0.9062    0.8286    0.8657        35\n",
      "        fear     0.8846    0.9200    0.9020        25\n",
      "\n",
      "    accuracy                         0.8992       119\n",
      "   macro avg     0.8941    0.8987    0.8957       119\n",
      "weighted avg     0.8996    0.8992    0.8987       119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval().cuda()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        X,y = batch_\n",
    "        input_ids = X[0]\n",
    "        attention_mask = X[1]\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        y_pred = torch.argmax(module(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(true_y, pred_y, labels=range(len(labels)))\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cm, annot=True, cmap='Greens', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save model\n",
    "# module.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model test\n",
    "# hparams = Namespace(\n",
    "#     train_path=train_path,\n",
    "#     val_path=val_path,\n",
    "#     test_path=test_path,\n",
    "#     batch_size=10,\n",
    "#     warmup_steps=100,\n",
    "#     epochs=20,\n",
    "#     lr=2.5E-05,\n",
    "#     accumulate_grad_batches=1\n",
    "# )\n",
    "# device = torch.device('cuda:0')\n",
    "# model = TrainingModule(hparams)\n",
    "# model.model.load_state_dict(torch.load('empathy_model\\BERT_emotion_1ft.pt'), strict=False)\n",
    "# model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
