{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import logging\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "from argparse import Namespace\n",
    "from packaging import version\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is partly adapted from the link below:\n",
    "# https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"emotion_data/my_train.txt\"\n",
    "test_path = \"emotion_data/my_test.txt\"\n",
    "val_path = \"emotion_data/my_val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary associating each string label to an integer value\n",
    "\n",
    "labels = [ \"sadness\", \"joy\", \"anger\", \"fear\"]\n",
    "label2int = dict(zip(labels, list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i gotta say for the first time in a long while...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am pissed</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im kind of firm as the school year is coming t...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today im really content</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im not fine my team was disqualified and i los...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    class\n",
       "0  i gotta say for the first time in a long while...      joy\n",
       "1                                        i am pissed    anger\n",
       "2  im kind of firm as the school year is coming t...  sadness\n",
       "3                            today im really content      joy\n",
       "4  im not fine my team was disqualified and i los...    anger"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"emotion_data/my_train.txt\",sep=\";\", header=None)\n",
    "data1 = pd.read_csv(\"emotion_data/my_val.txt\",sep=\";\", header=None)\n",
    "data2 = pd.read_csv(\"emotion_data/my_test.txt\",sep=\";\", header=None)\n",
    "data.columns = [\"text\",\"class\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoDataset(Dataset):\n",
    "  def __init__(\n",
    "    self,\n",
    "    path,\n",
    "    tokenizer: DistilBertTokenizer,\n",
    "    max_token_len: int = 100\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data_column = \"text\"\n",
    "    self.class_column = \"class\"\n",
    "    self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                            engine=\"python\")\n",
    "    \n",
    "    self.max_token_len = max_token_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "    text = data_row.text\n",
    "    labels = label2int[data_row[\"class\"]]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return (encoding[\"input_ids\"].flatten(),encoding[\"attention_mask\"].flatten()), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 101, 2026, 4654, 9868, 2026, 2166, 1998, 1045, 2572, 5506,  102,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0])),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EmoDataset(\n",
    "    train_path,\n",
    "    tokenizer,\n",
    "    max_token_len=100\n",
    ")\n",
    "sample_item = train_dataset[5]\n",
    "sample_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[1]\n",
    "# sample_item[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 100]), torch.Size([16, 100]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(DataLoader(train_dataset, batch_size=16)))\n",
    "sample_batch[0][0].shape, sample_batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(sample_batch[0][0], sample_batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.1031,  0.2227,  0.0414,  ...,  0.0191,  0.2575,  0.3722],\n",
       "         [ 0.3046,  0.2739,  0.1756,  ...,  0.0326,  0.5295,  0.6913],\n",
       "         [ 1.0214,  0.3050,  0.3467,  ..., -0.0498,  0.2737,  0.9548],\n",
       "         ...,\n",
       "         [-0.1791, -0.0794,  0.3395,  ...,  0.0381, -0.1493, -0.2007],\n",
       "         [-0.2979,  0.0012,  0.2718,  ...,  0.0012, -0.2241,  0.0244],\n",
       "         [-0.3091,  0.1529,  0.2534,  ..., -0.0766, -0.1674,  0.0195]],\n",
       "\n",
       "        [[ 0.0606,  0.1232,  0.1519,  ...,  0.0015,  0.3208,  0.3495],\n",
       "         [ 0.3743,  0.2497,  0.0587,  ..., -0.0790,  0.5911,  0.4906],\n",
       "         [ 0.0889,  0.5393,  0.5102,  ..., -0.0486,  0.4431,  0.5649],\n",
       "         ...,\n",
       "         [ 0.2955,  0.2265,  0.0643,  ...,  0.2148, -0.0127,  0.1539],\n",
       "         [ 0.3096,  0.2164,  0.0705,  ...,  0.2262, -0.0109,  0.1525],\n",
       "         [ 0.2522,  0.2623,  0.0570,  ...,  0.1941,  0.0059,  0.1437]],\n",
       "\n",
       "        [[ 0.0357,  0.3292,  0.1219,  ..., -0.0315,  0.3737,  0.4592],\n",
       "         [-0.3581,  0.6456,  0.7553,  ..., -0.1933,  0.1127,  0.7352],\n",
       "         [-0.2767, -0.1363,  0.4714,  ..., -0.3079, -0.1074,  0.2657],\n",
       "         ...,\n",
       "         [ 0.0053, -0.1851,  0.4727,  ..., -0.2847,  0.0300,  0.1591],\n",
       "         [ 0.2055,  0.3568,  0.4624,  ..., -0.0877, -0.0408,  0.2382],\n",
       "         [ 0.1898,  0.4620,  0.4627,  ..., -0.0245,  0.0222,  0.3293]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0475, -0.0390, -0.1564,  ..., -0.1425,  0.3534,  0.5742],\n",
       "         [ 0.2322,  0.1735, -0.3653,  ..., -0.0922,  0.4780,  0.4831],\n",
       "         [-0.0463,  0.5949,  0.1789,  ..., -0.3380,  0.0609,  0.2303],\n",
       "         ...,\n",
       "         [-0.2659,  0.2450,  0.4046,  ..., -0.1291, -0.0467,  0.0308],\n",
       "         [-0.4293,  0.1791,  0.5439,  ..., -0.1267,  0.0194,  0.1685],\n",
       "         [-0.3817,  0.2818,  0.4505,  ..., -0.1540,  0.1080,  0.0922]],\n",
       "\n",
       "        [[-0.0373,  0.0066, -0.1224,  ..., -0.0452,  0.1479,  0.2477],\n",
       "         [ 0.1611,  0.4942, -0.2769,  ...,  0.1155,  0.6888,  0.1825],\n",
       "         [-0.0274,  0.3853,  0.0277,  ..., -0.1029,  0.2236, -0.2037],\n",
       "         ...,\n",
       "         [-0.1130,  0.2546,  0.0110,  ..., -0.1537, -0.4221, -0.1416],\n",
       "         [-0.2265,  0.0210, -0.0088,  ..., -0.1325, -0.2170, -0.0140],\n",
       "         [ 0.1586, -0.0997, -0.0554,  ..., -0.0136,  0.0408,  0.1091]],\n",
       "\n",
       "        [[-0.1349,  0.1387,  0.2027,  ..., -0.0950,  0.3045,  0.4776],\n",
       "         [-0.3624,  0.4377,  1.0052,  ..., -0.1472,  0.3277,  0.9153],\n",
       "         [-0.4536, -0.2421,  0.3762,  ..., -0.3135, -0.1104,  0.2512],\n",
       "         ...,\n",
       "         [ 0.0196,  0.0212,  0.2927,  ...,  0.1689, -0.0049, -0.1008],\n",
       "         [ 0.0199,  0.0222,  0.3020,  ...,  0.1756,  0.0228, -0.1288],\n",
       "         [-0.1447,  0.2735,  0.5079,  ...,  0.0324, -0.0471,  0.1189]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom classifcation model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Mish activation function \n",
    "#(from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py)\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "  \n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an EmoClassificationModel class to do the actual fine-tuning\n",
    "\n",
    "class EmoClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "        \n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, *args):\n",
    "\n",
    "        hidden_states = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch Lightning for training.\n",
    "#we use PyTorch Lighning for training. Lightning methods are defined here\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmoClassificationModel(DistilBertModel.from_pretrained('distilbert-base-uncased'), len(labels)) #was \"distilroberta-base\"\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.max_token_length = 128\n",
    "        self.loss = nn.CrossEntropyLoss() #cross entropy loss since this is multi-class classification\n",
    "        # self.save_hyperparameters(hparams)\n",
    "        self.hparams = hparams\n",
    "        self.loss_amount = 0.\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        loss = self.loss(self.forward(input_ids=X[0], attention_mask = X[1]), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        \n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmoDataset(ds_path, self.tokenizer, self.max_token_length),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr) #we use AdamW as this usually performs well\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "   \n",
    "    # def save_model(self):\n",
    "    #     torch.save(self.model.state_dict(), 'emotion_model/BERT_emotion_1ft.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    }
   ],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=20,\n",
    "    warmup_steps=100,\n",
    "    epochs=30,\n",
    "    lr=2E-06,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)\n",
    "#rubbish collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # monitor validation loss\n",
    "    min_delta=0.001, #to very small change in the monitored quantity to qualify as an improvement\n",
    "    patience=20, # used to check number of time with no improvement after which training will be stopped\n",
    "    verbose=False, \n",
    "    mode=\"min\" #sed while training will stopped when the quantity monitor has stopped decreasing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:lightning:\n",
      "   | Name                                                   | Type                   | Params\n",
      "----------------------------------------------------------------------------------------------\n",
      "0  | model                                                  | EmoClassificationModel | 66 M  \n",
      "1  | model.base_model                                       | DistilBertModel        | 66 M  \n",
      "2  | model.base_model.embeddings                            | Embeddings             | 23 M  \n",
      "3  | model.base_model.embeddings.word_embeddings            | Embedding              | 23 M  \n",
      "4  | model.base_model.embeddings.position_embeddings        | Embedding              | 393 K \n",
      "5  | model.base_model.embeddings.LayerNorm                  | LayerNorm              | 1 K   \n",
      "6  | model.base_model.embeddings.dropout                    | Dropout                | 0     \n",
      "7  | model.base_model.transformer                           | Transformer            | 42 M  \n",
      "8  | model.base_model.transformer.layer                     | ModuleList             | 42 M  \n",
      "9  | model.base_model.transformer.layer.0                   | TransformerBlock       | 7 M   \n",
      "10 | model.base_model.transformer.layer.0.attention         | MultiHeadSelfAttention | 2 M   \n",
      "11 | model.base_model.transformer.layer.0.attention.dropout | Dropout                | 0     \n",
      "12 | model.base_model.transformer.layer.0.attention.q_lin   | Linear                 | 590 K \n",
      "13 | model.base_model.transformer.layer.0.attention.k_lin   | Linear                 | 590 K \n",
      "14 | model.base_model.transformer.layer.0.attention.v_lin   | Linear                 | 590 K \n",
      "15 | model.base_model.transformer.layer.0.attention.out_lin | Linear                 | 590 K \n",
      "16 | model.base_model.transformer.layer.0.sa_layer_norm     | LayerNorm              | 1 K   \n",
      "17 | model.base_model.transformer.layer.0.ffn               | FFN                    | 4 M   \n",
      "18 | model.base_model.transformer.layer.0.ffn.dropout       | Dropout                | 0     \n",
      "19 | model.base_model.transformer.layer.0.ffn.lin1          | Linear                 | 2 M   \n",
      "20 | model.base_model.transformer.layer.0.ffn.lin2          | Linear                 | 2 M   \n",
      "21 | model.base_model.transformer.layer.0.ffn.activation    | GELUActivation         | 0     \n",
      "22 | model.base_model.transformer.layer.0.output_layer_norm | LayerNorm              | 1 K   \n",
      "23 | model.base_model.transformer.layer.1                   | TransformerBlock       | 7 M   \n",
      "24 | model.base_model.transformer.layer.1.attention         | MultiHeadSelfAttention | 2 M   \n",
      "25 | model.base_model.transformer.layer.1.attention.dropout | Dropout                | 0     \n",
      "26 | model.base_model.transformer.layer.1.attention.q_lin   | Linear                 | 590 K \n",
      "27 | model.base_model.transformer.layer.1.attention.k_lin   | Linear                 | 590 K \n",
      "28 | model.base_model.transformer.layer.1.attention.v_lin   | Linear                 | 590 K \n",
      "29 | model.base_model.transformer.layer.1.attention.out_lin | Linear                 | 590 K \n",
      "30 | model.base_model.transformer.layer.1.sa_layer_norm     | LayerNorm              | 1 K   \n",
      "31 | model.base_model.transformer.layer.1.ffn               | FFN                    | 4 M   \n",
      "32 | model.base_model.transformer.layer.1.ffn.dropout       | Dropout                | 0     \n",
      "33 | model.base_model.transformer.layer.1.ffn.lin1          | Linear                 | 2 M   \n",
      "34 | model.base_model.transformer.layer.1.ffn.lin2          | Linear                 | 2 M   \n",
      "35 | model.base_model.transformer.layer.1.output_layer_norm | LayerNorm              | 1 K   \n",
      "36 | model.base_model.transformer.layer.2                   | TransformerBlock       | 7 M   \n",
      "37 | model.base_model.transformer.layer.2.attention         | MultiHeadSelfAttention | 2 M   \n",
      "38 | model.base_model.transformer.layer.2.attention.dropout | Dropout                | 0     \n",
      "39 | model.base_model.transformer.layer.2.attention.q_lin   | Linear                 | 590 K \n",
      "40 | model.base_model.transformer.layer.2.attention.k_lin   | Linear                 | 590 K \n",
      "41 | model.base_model.transformer.layer.2.attention.v_lin   | Linear                 | 590 K \n",
      "42 | model.base_model.transformer.layer.2.attention.out_lin | Linear                 | 590 K \n",
      "43 | model.base_model.transformer.layer.2.sa_layer_norm     | LayerNorm              | 1 K   \n",
      "44 | model.base_model.transformer.layer.2.ffn               | FFN                    | 4 M   \n",
      "45 | model.base_model.transformer.layer.2.ffn.dropout       | Dropout                | 0     \n",
      "46 | model.base_model.transformer.layer.2.ffn.lin1          | Linear                 | 2 M   \n",
      "47 | model.base_model.transformer.layer.2.ffn.lin2          | Linear                 | 2 M   \n",
      "48 | model.base_model.transformer.layer.2.output_layer_norm | LayerNorm              | 1 K   \n",
      "49 | model.base_model.transformer.layer.3                   | TransformerBlock       | 7 M   \n",
      "50 | model.base_model.transformer.layer.3.attention         | MultiHeadSelfAttention | 2 M   \n",
      "51 | model.base_model.transformer.layer.3.attention.dropout | Dropout                | 0     \n",
      "52 | model.base_model.transformer.layer.3.attention.q_lin   | Linear                 | 590 K \n",
      "53 | model.base_model.transformer.layer.3.attention.k_lin   | Linear                 | 590 K \n",
      "54 | model.base_model.transformer.layer.3.attention.v_lin   | Linear                 | 590 K \n",
      "55 | model.base_model.transformer.layer.3.attention.out_lin | Linear                 | 590 K \n",
      "56 | model.base_model.transformer.layer.3.sa_layer_norm     | LayerNorm              | 1 K   \n",
      "57 | model.base_model.transformer.layer.3.ffn               | FFN                    | 4 M   \n",
      "58 | model.base_model.transformer.layer.3.ffn.dropout       | Dropout                | 0     \n",
      "59 | model.base_model.transformer.layer.3.ffn.lin1          | Linear                 | 2 M   \n",
      "60 | model.base_model.transformer.layer.3.ffn.lin2          | Linear                 | 2 M   \n",
      "61 | model.base_model.transformer.layer.3.output_layer_norm | LayerNorm              | 1 K   \n",
      "62 | model.base_model.transformer.layer.4                   | TransformerBlock       | 7 M   \n",
      "63 | model.base_model.transformer.layer.4.attention         | MultiHeadSelfAttention | 2 M   \n",
      "64 | model.base_model.transformer.layer.4.attention.dropout | Dropout                | 0     \n",
      "65 | model.base_model.transformer.layer.4.attention.q_lin   | Linear                 | 590 K \n",
      "66 | model.base_model.transformer.layer.4.attention.k_lin   | Linear                 | 590 K \n",
      "67 | model.base_model.transformer.layer.4.attention.v_lin   | Linear                 | 590 K \n",
      "68 | model.base_model.transformer.layer.4.attention.out_lin | Linear                 | 590 K \n",
      "69 | model.base_model.transformer.layer.4.sa_layer_norm     | LayerNorm              | 1 K   \n",
      "70 | model.base_model.transformer.layer.4.ffn               | FFN                    | 4 M   \n",
      "71 | model.base_model.transformer.layer.4.ffn.dropout       | Dropout                | 0     \n",
      "72 | model.base_model.transformer.layer.4.ffn.lin1          | Linear                 | 2 M   \n",
      "73 | model.base_model.transformer.layer.4.ffn.lin2          | Linear                 | 2 M   \n",
      "74 | model.base_model.transformer.layer.4.output_layer_norm | LayerNorm              | 1 K   \n",
      "75 | model.base_model.transformer.layer.5                   | TransformerBlock       | 7 M   \n",
      "76 | model.base_model.transformer.layer.5.attention         | MultiHeadSelfAttention | 2 M   \n",
      "77 | model.base_model.transformer.layer.5.attention.dropout | Dropout                | 0     \n",
      "78 | model.base_model.transformer.layer.5.attention.q_lin   | Linear                 | 590 K \n",
      "79 | model.base_model.transformer.layer.5.attention.k_lin   | Linear                 | 590 K \n",
      "80 | model.base_model.transformer.layer.5.attention.v_lin   | Linear                 | 590 K \n",
      "81 | model.base_model.transformer.layer.5.attention.out_lin | Linear                 | 590 K \n",
      "82 | model.base_model.transformer.layer.5.sa_layer_norm     | LayerNorm              | 1 K   \n",
      "83 | model.base_model.transformer.layer.5.ffn               | FFN                    | 4 M   \n",
      "84 | model.base_model.transformer.layer.5.ffn.dropout       | Dropout                | 0     \n",
      "85 | model.base_model.transformer.layer.5.ffn.lin1          | Linear                 | 2 M   \n",
      "86 | model.base_model.transformer.layer.5.ffn.lin2          | Linear                 | 2 M   \n",
      "87 | model.base_model.transformer.layer.5.output_layer_norm | LayerNorm              | 1 K   \n",
      "88 | model.classifier                                       | Sequential             | 593 K \n",
      "89 | model.classifier.0                                     | Dropout                | 0     \n",
      "90 | model.classifier.1                                     | Linear                 | 590 K \n",
      "91 | model.classifier.2                                     | Mish                   | 0     \n",
      "92 | model.classifier.3                                     | Dropout                | 0     \n",
      "93 | model.classifier.4                                     | Linear                 | 3 K   \n",
      "94 | loss                                                   | CrossEntropyLoss       | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  74%|███████▍  | 40/54 [00:11<00:04,  3.46it/s, loss=0.177, train_loss=0.0638, v_num=167]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train (using cuda)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "                     early_stop_callback=early_stop_callback)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval().cuda()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        X,y = batch_\n",
    "        input_ids = X[0]\n",
    "        attention_mask = X[1]\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        y_pred = torch.argmax(module(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(true_y, pred_y, labels=range(len(labels)))\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cm, annot=True, cmap='Greens', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save model\n",
    "# module.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model test\n",
    "# hparams = Namespace(\n",
    "#     train_path=train_path,\n",
    "#     val_path=val_path,\n",
    "#     test_path=test_path,\n",
    "#     batch_size=10,\n",
    "#     warmup_steps=100,\n",
    "#     epochs=20,\n",
    "#     lr=2.5E-05,\n",
    "#     accumulate_grad_batches=1\n",
    "# )\n",
    "# device = torch.device('cuda:0')\n",
    "# model = TrainingModule(hparams)\n",
    "# model.model.load_state_dict(torch.load('empathy_model\\BERT_emotion_1ft.pt'), strict=False)\n",
    "# model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
