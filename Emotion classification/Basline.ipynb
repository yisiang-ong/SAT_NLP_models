{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Empathetic Personas dataset by (Lisa et al. 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITATION = \"\"\"@article{Alazraki2022,\n",
    "    title = {{An Empathetic AI Coach for Self-Attachment Therapy}},\n",
    "    year = {2022},\n",
    "    author = {Alazraki, Lisa and Ghachem, Ali and Polydorou, Neophytos and Khosmood, Foaad and Edalat, Abbas},\n",
    "    pages = {78--87},\n",
    "    isbn = {9781665416214},\n",
    "    doi = {10.1109/cogmi52975.2021.00019},\n",
    "    keywords = {-digital psychotherapy, agents in a sensitive, area such as mental, chatbots, current deep-learning approaches to, healthcare poses, however, it should be noted, self-attachment, significant challenges, that using conversational}\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import empathetic Personas dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\yisiang\\\\OneDrive\\\\Desktop\\\\MSc AI\\\\007 MSC AI Individual\\\\SATbot3.0\\\\datasets\\\\empatheticPersonas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_df = data[['Sad - Patient response 1', 'Sad - Patient response 2', 'Sad - Patient response 3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract each column as series\n",
    "sad1 = sad_df['Sad - Patient response 1']\n",
    "sad2 = sad_df['Sad - Patient response 2']\n",
    "sad3 = sad_df['Sad - Patient response 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate series\n",
    "sad12 = sad1.append(sad2)\n",
    "sad = sad12.append(sad3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert result to dataframe and drop rows with null values\n",
    "sad_df = sad.to_frame()\n",
    "sad_df = sad_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert class column with all values set to sadness and rename columns\n",
    "sad_df.insert(1, 'class', 'sadness')\n",
    "sad_df.columns = ['text', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract angry patient response columns\n",
    "angry_df = data[['Angry - Patient response 1', 'Angry - Patient response 2', 'Angry - Patient response 3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract each angry column as series\n",
    "angry1 = angry_df['Angry - Patient response 1']\n",
    "angry2 = angry_df['Angry - Patient response 2']\n",
    "angry3 = angry_df['Angry - Patient response 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate series\n",
    "angry12 = angry1.append(angry2)\n",
    "angry = angry12.append(angry3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert angry series to dataframe and drop rows with null values\n",
    "angry_df = angry.to_frame()\n",
    "angry_df = angry_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert class column with all values set to anger and rename columns\n",
    "angry_df.insert(1, 'class', 'anger')\n",
    "angry_df.columns = ['text', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract anxious/fearful patient response columns\n",
    "fearful_df = data[['Anxious - Patient response 1', 'Anxious - Patient response 2', 'Anxious - Patient response 3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract each fearful column as series\n",
    "fearful1 = fearful_df['Anxious - Patient response 1']\n",
    "fearful2 = fearful_df['Anxious - Patient response 2']\n",
    "fearful3 = fearful_df['Anxious - Patient response 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate series\n",
    "fearful12 = fearful1.append(fearful2)\n",
    "fearful = fearful12.append(fearful3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert fearful series to dataframe and drop rows with null values\n",
    "fearful_df = fearful.to_frame()\n",
    "fearful_df = fearful_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert class column with all values set to fear and rename columns\n",
    "fearful_df.insert(1, 'class', 'fear')\n",
    "fearful_df.columns = ['text', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract happy patient response columns\n",
    "happy_df = data[['Happy - Patient response 1', 'Happy - Patient response 2', 'Happy - Patient response 3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract each happy column as series\n",
    "happy1 = happy_df['Happy - Patient response 1']\n",
    "happy2 = happy_df['Happy - Patient response 2']\n",
    "happy3 = happy_df['Happy - Patient response 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate series\n",
    "happy12 = happy1.append(happy2)\n",
    "happy = happy12.append(happy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert happy series to dataframe and drop rows with null values\n",
    "happy_df = happy.to_frame()\n",
    "happy_df = happy_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert class column with all values set to joy and rename columns\n",
    "happy_df.insert(1, 'class', 'joy')\n",
    "happy_df.columns = ['text', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate sad, angry, fearful and happy dataframes into one, shuffle and reset indices\n",
    "df = pd.concat([sad_df, angry_df, fearful_df, happy_df])\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the classes are well distributed\n",
    "df.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation from responses in text column and make everything lower case\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'class': 'emotions'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"emotion_data/my_train.txt\" \n",
    "test_path = \"emotion_data/my_test.txt\"\n",
    "val_path = \"emotion_data/my_val.txt\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_train, input_val, target_train, target_val = train_test_split(df.text.to_numpy(), \n",
    "                                                                    df.emotions.to_numpy(), \n",
    "                                                                    test_size=0.2)\n",
    "\n",
    "# Split the validataion further to obtain a holdout dataset (for testing) -- split 1:1\n",
    "input_val, input_test, target_val, target_test = train_test_split(input_val, target_val, test_size=0.5)\n",
    "\n",
    "\n",
    "## create a dataframe for each dataset\n",
    "train_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": target_train})\n",
    "val_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": target_val})\n",
    "test_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": target_test})\n",
    "final_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\n",
    "\n",
    "train_dataset.to_csv(train_path, sep=\";\",header=False, index=False)\n",
    "val_dataset.to_csv(val_path, sep=\";\",header=False, index=False)\n",
    "test_dataset.to_csv(test_path, sep=\";\",header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "We use the rule-based method as baseline which uses lists of for and against keyword to label emotions. This is the method which was implemented by imperial student Ali Ghachem in the previous SATbot.\n",
    "\n",
    "The for and against lists contain synonyms and antonyms, respectively, of the target emotions, augmented using wordnet. For each emotion we count the number of for words and the number of against words, then subtract one from the other. The result is the score for that emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can test this method directly on our test set\n",
    "\n",
    "test_path = \"emotion_data/my_test.txt\" #change path if necessary to match previous changes\n",
    "test_dataset = pd.read_csv(test_path, sep=';', header=None)\n",
    "test_dataset.columns = ['text', 'class']\n",
    "\n",
    "#define the functions we need (these are adapted from Ali Ghachem's original code)\n",
    "\n",
    "def calculate_emotion_scores(user_response):\n",
    "\n",
    "        user_input = user_response.lower()\n",
    "        tokenised_input = re.findall(r\"[\\w']+\", user_input)\n",
    "\n",
    "        scores = [\n",
    "            detect_happy(tokenised_input),\n",
    "            detect_sad(tokenised_input),\n",
    "            detect_angry(tokenised_input),\n",
    "            detect_anxious(tokenised_input)\n",
    "        ]\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "def detect_happy(tokenised_input):\n",
    "\n",
    "        starting_for_keywords = [\n",
    "            \"good\",\n",
    "            \"happy\",\n",
    "            \"fine\",\n",
    "            \"well\",\n",
    "            \"delighted\",\n",
    "            \"great\",\n",
    "            \"positive\",\n",
    "            \"decent\",\n",
    "            \"content\",\n",
    "            \"relaxed\",\n",
    "            \"ok\",\n",
    "            \"joyful\"\n",
    "        ]\n",
    "        starting_against_keywords = [\n",
    "            \"sad\",\n",
    "            \"unhappy\",\n",
    "            \"angry\",\n",
    "            \"scared\",\n",
    "            \"anxious\",\n",
    "            \"upset\"\n",
    "        ]\n",
    "\n",
    "        #keywords for detecting and not detecting the emotion, respectively\n",
    "        for_keywords, against_keywords = extract_for_and_against_keywords(\n",
    "            starting_for_keywords, starting_against_keywords\n",
    "        )\n",
    "\n",
    "        return calculate_score(tokenised_input, for_keywords, against_keywords)\n",
    "\n",
    "\n",
    "def detect_sad(tokenised_input):\n",
    "        starting_for_keywords = [\"bad\", \"sad\", \"unhappy\", \"depressed\"]\n",
    "        starting_against_keywords = [\n",
    "            \"neutral\",\n",
    "            \"angry\",\n",
    "            \"scared\",\n",
    "            \"anxious\",\n",
    "            \"happy\",\n",
    "            \"worried\",\n",
    "            \"not\",\n",
    "        ]\n",
    "\n",
    "        # Keywords for detecting and not detecting the behaviour, respectively\n",
    "        for_keywords, against_keywords = extract_for_and_against_keywords(\n",
    "            starting_for_keywords, starting_against_keywords\n",
    "        )\n",
    "\n",
    "        return calculate_score(tokenised_input, for_keywords, against_keywords)\n",
    "\n",
    "\n",
    "def detect_angry(tokenised_input):\n",
    "        starting_for_keywords = [\"angry\", \"furious\", \"rage\", \"enraged\"]\n",
    "        starting_against_keywords = [\n",
    "            \"neutral\",\n",
    "            \"upset\",\n",
    "            \"depressed\",\n",
    "            \"scared\",\n",
    "            \"anxious\",\n",
    "            \"worried\",\n",
    "            \"happy\",\n",
    "        ]\n",
    "\n",
    "        # Keywords for detecting and not detecting the behaviour, respectively\n",
    "        for_keywords, against_keywords = extract_for_and_against_keywords(\n",
    "            starting_for_keywords, starting_against_keywords\n",
    "        )\n",
    "\n",
    "        return calculate_score(tokenised_input, for_keywords, against_keywords)\n",
    "\n",
    "\n",
    "def detect_anxious(tokenised_input):\n",
    "        starting_for_keywords = [\"scared\", \"afraid\", \"anxious\", \"worried\", \"fearful\"]\n",
    "        starting_against_keywords = [\"neutral\", \"upset\", \"depressed\", \"happy\", \"angry\", \"furious\"]\n",
    "\n",
    "        # Keywords for detecting and not detecting the behaviour, respectively\n",
    "        for_keywords, against_keywords = extract_for_and_against_keywords(\n",
    "            starting_for_keywords, starting_against_keywords\n",
    "        )\n",
    "\n",
    "        return calculate_score(tokenised_input, for_keywords, against_keywords)\n",
    "\n",
    "\n",
    "def extract_for_and_against_keywords(\n",
    "        starting_for_keywords, starting_against_keywords\n",
    "    ):\n",
    "        for_keywords = starting_for_keywords\n",
    "        against_keywords = starting_against_keywords\n",
    "        for i in range(len(starting_for_keywords)):\n",
    "            word = starting_for_keywords[i]\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for lemma in syn.lemmas():\n",
    "                    # Adds lemmas to for_keywords\n",
    "                    for_keywords.append(lemma.name())\n",
    "                    if lemma.antonyms():\n",
    "                        for antonym in lemma.antonyms():\n",
    "                            against_keywords.append(antonym.name())\n",
    "\n",
    "        return set(for_keywords), set(against_keywords)\n",
    "\n",
    "\n",
    "def calculate_score(user_input, for_keywords, against_keywords):\n",
    "\n",
    "        score = 0\n",
    "        for word in user_input:\n",
    "            if word in for_keywords:\n",
    "                score += 1\n",
    "\n",
    "            elif word in against_keywords:\n",
    "                score -= 1\n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_rulebased(text):\n",
    "\n",
    "  emotion_scores = calculate_emotion_scores(text)\n",
    "  max_pos = np.argmax(emotion_scores)\n",
    "  emotion = ''\n",
    "\n",
    "  if max_pos == 0:\n",
    "    emotion = 'joy'\n",
    "  elif max_pos == 1:\n",
    "    emotion = 'sadness'\n",
    "  elif max_pos == 2:\n",
    "    emotion = 'anger'\n",
    "  else:\n",
    "    emotion = 'fear'\n",
    "\n",
    "  return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick sanity check before testing on the whole test set\n",
    "text=\"I feel like nothing matters anymore\" #we try a sentence with no sentiment word to trick the model, but the sentiment is clear (sadness)\n",
    "get_emotion_rulebased(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we test this on our test set (the same we will use to test the finetuned transformer model)\n",
    "\n",
    "test_dataset['pred'] = ''\n",
    "\n",
    "for index, row in test_dataset.iterrows():\n",
    "  test_dataset.at[index, 'pred'] = get_emotion_rulebased(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = test_dataset['pred'].to_numpy().tolist()\n",
    "targets = test_dataset['class'].to_numpy().tolist()\n",
    "\n",
    "#check all metrics\n",
    "print(metrics.classification_report(targets, outputs, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = metrics.confusion_matrix(targets, outputs)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [\"anger\", \"fear\", \"joy\", \"sadness\"], columns = [\"anger\", \"fear\", \"joy\", \"sadness\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, cmap='Greens', fmt='g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
