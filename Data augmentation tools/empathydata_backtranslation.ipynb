{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BacktranslationMachine:\n",
    "    def __init__(self, src=\"en\", tgt=\"de\"):\n",
    "        # Languages code: https://developers.google.com/admin-sdk/directory/v1/languages \n",
    "        # https://towardsdatascience.com/data-augmentation-in-nlp-using-back-translation-with-marianmt-a8939dfea50a\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        \n",
    "        self.tokenizer1 = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{src}-{tgt}\")  \n",
    "        self.model1 = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{src}-{tgt}\")\n",
    "\n",
    "        self.tokenizer2 = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{tgt}-{src}\")  \n",
    "        self.model2 = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{tgt}-{src}\")\n",
    "\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.model1 = self.model1.to(self.device)\n",
    "        self.model2 = self.model2.to(self.device)\n",
    "        \n",
    "\n",
    "    def process_text(self, lang_code, text):\n",
    "        formatted_text = [f\">>{lang_code}<< {t}\" for t in text]\n",
    "        return formatted_text\n",
    "\n",
    "\n",
    "    def translation1(self, input_sentence):\n",
    "        # translate to second language\n",
    "        formatted_text = self.process_text(self.tgt, input_sentence)\n",
    "        encoded_lang1 = self.tokenizer1(formatted_text, padding=True, return_tensors=\"pt\")\n",
    "        translated_encoded_lang1 = self.model1.generate(**encoded_lang1)\n",
    "        decoded_lang2 = self.tokenizer1.batch_decode(translated_encoded_lang1, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        return decoded_lang2\n",
    "\n",
    "    def translation2(self, input_sentence):\n",
    "        # translate back to first language\n",
    "        formatted_text = self.process_text(self.src, input_sentence)\n",
    "        encoded_lang2 = self.tokenizer2(formatted_text, padding=True, return_tensors=\"pt\")\n",
    "        translated_encoded_lang1 = self.model2.generate(**encoded_lang2)\n",
    "        decoded_lang1 = self.tokenizer2.batch_decode(translated_encoded_lang1, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        return decoded_lang1\n",
    "\n",
    "    def backtranslation(self, input_sentence):\n",
    "        translated_sentence = self.translation1(input_sentence)\n",
    "        backtranslated_sentence = self.translation2(translated_sentence)\n",
    "        return backtranslated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"en\"\n",
    "tgt = \"fi\" # de, zh, \n",
    "bm = BacktranslationMachine(src=src, tgt=tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_sentence = [\"Sometimes it can be difficult to think of a creative domain that you'd like to pursue, but it's all right and don't worry! We'll figure it out for you. Here are a few exercises you can do to reconnect to your childhood and ignite a creative domain.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sometimes it may be difficult to think about the creative area that you want to pursue, but it’s all right! We’ll figure it out for you. Here are some exercises you can do to get back to childhood and set the creative area on fire.']\n"
     ]
    }
   ],
   "source": [
    "bm_generated_sentence = bm.backtranslation(input_sentence)\n",
    "print(bm_generated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
