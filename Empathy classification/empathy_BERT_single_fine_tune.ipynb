{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from typing import List\n",
    "import logging\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "from argparse import Namespace\n",
    "from packaging import version\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is partly adapted from the link below:\n",
    "# https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"empathy_dataset/my_train.txt\"\n",
    "test_path = \"empathy_dataset/my_test.txt\"\n",
    "val_path = \"empathy_dataset/my_val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weak': 0, 'strong': 1}\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary associating each string label to an integer value\n",
    "\n",
    "labels = [\"weak\", \"strong\"]\n",
    "label2int = dict(zip(labels, list(range(len(labels)))))\n",
    "print(label2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use bert-base-uncased\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why not try some of my suggestions of activiti...</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everybody struggles to think of a creative dom...</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you think of a creative task you wish to t...</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feel free to relax the muscles around your mou...</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would you like to do another round? I don't me...</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   class\n",
       "0  Why not try some of my suggestions of activiti...    weak\n",
       "1  Everybody struggles to think of a creative dom...  strong\n",
       "2  Can you think of a creative task you wish to t...  strong\n",
       "3  Feel free to relax the muscles around your mou...    weak\n",
       "4  Would you like to do another round? I don't me...    weak"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"empathy_dataset/my_train.txt\",sep=\";\", header=None)\n",
    "data.columns = [\"text\",\"class\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "strong    575\n",
       "weak      626\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know it may seem difficult but try now this {protocol}. Believe in the new side you are about to discover, when you feel right, hit Continue to move on.\n"
     ]
    }
   ],
   "source": [
    "samele_row = data.iloc[5]\n",
    "sample_text = samele_row.text\n",
    "# sample_text = \"I have a few exercises in mind that you could really benefit from and bursh up on {a dichotomy} you may be a little hesitant and this is okay, you can trust me and yourself!\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "encoding.keys()\n",
    "encoding[\"input_ids\"].shape, encoding[\"attention_mask\"].shape\n",
    "# print(encoding[\"input_ids\"].cpu().detach().numpy())\n",
    "# tokenizer.decode(encoding[\"input_ids\"].cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'know', 'it', 'may', 'seem', 'difficult', 'but', 'try', 'now', 'this', '{', 'protocol', '}', '.', 'believe', 'in', 'the', 'new', 'side']\n"
     ]
    }
   ],
   "source": [
    "encoding[\"input_ids\"].squeeze()[:20]\n",
    "print(tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sentence count')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "token_counts = []\n",
    "for _, row in data.iterrows():\n",
    "    token_count = len(tokenizer.encode(\n",
    "        row[\"text\"],\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ))\n",
    "    token_counts.append(token_count)\n",
    "sp1 = sns.histplot(token_counts)\n",
    "sp1.set_xlabel('Word count in a sentence')\n",
    "sp1.set_ylabel('Sentence count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_COUNT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpathyDataset(Dataset):\n",
    "  def __init__(\n",
    "    self,\n",
    "    path,\n",
    "    tokenizer: BertTokenizer,\n",
    "    max_token_len: int = 100\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data_column = \"text\"\n",
    "    self.class_column = \"class\"\n",
    "    self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                            engine=\"python\")\n",
    "    \n",
    "    self.max_token_len = max_token_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "    text = data_row.text\n",
    "    labels = label2int[data_row[\"class\"]]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return (encoding[\"input_ids\"].flatten(),encoding[\"attention_mask\"].flatten()), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 101, 1045, 2113, 2009, 2089, 4025, 3697, 2021, 3046, 2085, 2023, 1063,\n",
       "          8778, 1065, 1012, 2903, 1999, 1996, 2047, 2217, 2017, 2024, 2055, 2000,\n",
       "          7523, 1010, 2043, 2017, 2514, 2157, 1010, 2718, 3613, 2000, 2693, 2006,\n",
       "          1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0])),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EmpathyDataset(\n",
    "    train_path,\n",
    "    tokenizer,\n",
    "    max_token_len=100\n",
    ")\n",
    "sample_item = train_dataset[5]\n",
    "sample_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[1]\n",
    "# sample_item[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 100]), torch.Size([16, 100]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(DataLoader(train_dataset, batch_size=16)))\n",
    "sample_batch[0][0].shape, sample_batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert_model(sample_batch[0][0], sample_batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 100, 768]), torch.Size([16, 768]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 768 dimension comes from the BERT hidden size\n",
    "output.last_hidden_state.shape, output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0958, -0.2246, -0.0520,  ..., -0.1829,  0.2934,  0.6625],\n",
       "         [ 1.1400, -0.2461, -0.0313,  ..., -0.7879,  0.8866,  0.5441],\n",
       "         [ 0.4881, -0.9838, -0.1591,  ..., -0.0885,  0.2796,  0.2959],\n",
       "         ...,\n",
       "         [-0.0123, -0.2624,  0.2377,  ..., -0.2386, -0.1309,  0.1643],\n",
       "         [-0.1661, -0.3160,  0.0853,  ..., -0.3310, -0.1520,  0.3257],\n",
       "         [-0.0388,  0.0146,  0.2789,  ..., -0.4749,  0.0562,  0.4101]],\n",
       "\n",
       "        [[-0.2469, -0.3833, -0.1283,  ..., -0.2040,  0.4759,  0.6291],\n",
       "         [-0.2078,  0.7238, -0.2805,  ...,  0.2558,  0.6465,  0.5752],\n",
       "         [-0.4527,  0.9919,  0.6014,  ..., -0.0859, -0.1143,  0.2621],\n",
       "         ...,\n",
       "         [-0.3710, -0.7747,  0.0795,  ...,  0.2831,  0.2395, -0.0279],\n",
       "         [ 0.0336, -0.1207,  0.2034,  ...,  0.0973, -0.1647,  0.2645],\n",
       "         [ 0.2026,  0.1834,  0.1632,  ...,  0.5559, -0.0302,  0.5142]],\n",
       "\n",
       "        [[-0.0709, -0.2465, -0.2266,  ..., -0.3428,  0.2776,  0.5082],\n",
       "         [ 0.4718,  0.0395, -0.1676,  ...,  0.2162,  0.6435,  0.0138],\n",
       "         [-0.3174, -0.3842, -0.4098,  ...,  0.0708, -0.0773,  0.0545],\n",
       "         ...,\n",
       "         [ 0.1699, -0.2518,  0.0552,  ..., -0.2036, -0.2775,  0.0484],\n",
       "         [ 0.0596, -0.0859, -0.1703,  ..., -0.0703, -0.3317,  0.0440],\n",
       "         [ 0.0638, -0.0049, -0.2206,  ..., -0.0248, -0.2459,  0.0682]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2943, -0.0107, -0.4570,  ..., -0.3446,  0.5176,  0.7287],\n",
       "         [ 0.2717,  0.4333, -0.9591,  ..., -0.0820,  0.7632,  0.2016],\n",
       "         [ 1.0779, -0.1208, -0.5333,  ..., -0.1500,  0.2538,  0.1065],\n",
       "         ...,\n",
       "         [ 0.2307, -0.0709,  0.4494,  ..., -0.0195,  0.1417,  0.7183],\n",
       "         [ 0.2053, -0.1241,  0.4912,  ..., -0.0635,  0.1317,  0.7627],\n",
       "         [ 0.1620, -0.0596,  0.2416,  ..., -0.1392,  0.1688,  0.4955]],\n",
       "\n",
       "        [[ 0.0890, -0.1669,  0.2883,  ..., -0.3077,  0.3897,  0.8646],\n",
       "         [ 0.2835,  0.1435, -0.2152,  ...,  0.4482,  0.6150, -0.5064],\n",
       "         [ 0.6735,  0.3592,  0.6024,  ...,  0.2979,  0.2639, -0.3278],\n",
       "         ...,\n",
       "         [ 0.0718, -0.0299,  0.5311,  ..., -0.1377,  0.1418,  0.3568],\n",
       "         [ 0.2571,  0.3978,  0.4041,  ..., -0.0649,  0.2550,  0.3840],\n",
       "         [ 0.1882, -0.2760,  0.6177,  ...,  0.2536,  0.0335,  0.0170]],\n",
       "\n",
       "        [[ 0.1601, -0.4670, -0.1117,  ..., -0.4368, -0.1205,  0.8617],\n",
       "         [ 0.2784,  0.3703, -0.4463,  ..., -0.0680,  0.4972, -0.0368],\n",
       "         [ 0.3873,  0.3672,  0.1675,  ...,  0.0398,  0.4556, -1.1365],\n",
       "         ...,\n",
       "         [ 0.3080, -0.3350,  0.3401,  ...,  0.0061,  0.2132, -0.1924],\n",
       "         [ 0.2436, -0.1344,  0.4347,  ...,  0.2793, -0.0401,  0.1700],\n",
       "         [ 0.4237,  0.1025,  0.1415,  ...,  0.2119, -0.2366,  0.5099]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.7142, -0.3350, -0.8399,  ..., -0.6168, -0.6225,  0.7758],\n",
       "        [-0.6916, -0.4301, -0.9620,  ..., -0.9272, -0.6896,  0.5668],\n",
       "        [-0.8565, -0.3548, -0.8053,  ..., -0.7970, -0.7259,  0.8423],\n",
       "        ...,\n",
       "        [-0.8623, -0.5362, -0.9403,  ..., -0.9265, -0.7105,  0.7210],\n",
       "        [-0.8884, -0.3449, -0.8526,  ..., -0.6817, -0.6356,  0.9389],\n",
       "        [-0.8956, -0.4511, -0.9819,  ..., -0.9337, -0.7491,  0.7837]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom classifcation model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Mish activation function \n",
    "#(from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py)\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "  \n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an EmpathyClassificationModel class to do the actual fine-tuning\n",
    "\n",
    "class EmpathyClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "        \n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, *args):\n",
    "\n",
    "        hidden_states = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch Lightning for training.\n",
    "#we use PyTorch Lighning for training. Lightning methods are defined here\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmpathyClassificationModel(BertModel.from_pretrained(\"bert-base-uncased\"), len(labels)) #was \"distilroberta-base\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.max_token_length = 128\n",
    "        self.loss = nn.CrossEntropyLoss() #cross entropy loss since this is multi-class classification\n",
    "        # self.save_hyperparameters(hparams)\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        # print(self.forward(input_ids=X[0], attention_mask = X[1]))\n",
    "        loss = self.loss(self.forward(input_ids=X[0], attention_mask = X[1]), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        \n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmpathyDataset(ds_path, self.tokenizer, self.max_token_length),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr) #we use AdamW as this usually performs well\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "   \n",
    "    def save_model(self, version):\n",
    "        torch.save(self.model.state_dict(), f'empathy_model/BERT_empathy_1ft_{version}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=20,\n",
    "    warmup_steps=100,\n",
    "    epochs=50,\n",
    "    lr=2E-06,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)\n",
    "#rubbish collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # monitor validation loss\n",
    "    min_delta=0.001, #to very small change in the monitored quantity to qualify as an improvement\n",
    "    patience=20, # used to check number of time with no improvement after which training will be stopped\n",
    "    verbose=False, \n",
    "    mode=\"min\" #sed while training will stopped when the quantity monitor has stopped decreasing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:lightning:\n",
      "    | Name                                                              | Type                       | Params\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "0   | model                                                             | EmpathyClassificationModel | 110 M \n",
      "1   | model.base_model                                                  | BertModel                  | 109 M \n",
      "2   | model.base_model.embeddings                                       | BertEmbeddings             | 23 M  \n",
      "3   | model.base_model.embeddings.word_embeddings                       | Embedding                  | 23 M  \n",
      "4   | model.base_model.embeddings.position_embeddings                   | Embedding                  | 393 K \n",
      "5   | model.base_model.embeddings.token_type_embeddings                 | Embedding                  | 1 K   \n",
      "6   | model.base_model.embeddings.LayerNorm                             | LayerNorm                  | 1 K   \n",
      "7   | model.base_model.embeddings.dropout                               | Dropout                    | 0     \n",
      "8   | model.base_model.encoder                                          | BertEncoder                | 85 M  \n",
      "9   | model.base_model.encoder.layer                                    | ModuleList                 | 85 M  \n",
      "10  | model.base_model.encoder.layer.0                                  | BertLayer                  | 7 M   \n",
      "11  | model.base_model.encoder.layer.0.attention                        | BertAttention              | 2 M   \n",
      "12  | model.base_model.encoder.layer.0.attention.self                   | BertSelfAttention          | 1 M   \n",
      "13  | model.base_model.encoder.layer.0.attention.self.query             | Linear                     | 590 K \n",
      "14  | model.base_model.encoder.layer.0.attention.self.key               | Linear                     | 590 K \n",
      "15  | model.base_model.encoder.layer.0.attention.self.value             | Linear                     | 590 K \n",
      "16  | model.base_model.encoder.layer.0.attention.self.dropout           | Dropout                    | 0     \n",
      "17  | model.base_model.encoder.layer.0.attention.output                 | BertSelfOutput             | 592 K \n",
      "18  | model.base_model.encoder.layer.0.attention.output.dense           | Linear                     | 590 K \n",
      "19  | model.base_model.encoder.layer.0.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "20  | model.base_model.encoder.layer.0.attention.output.dropout         | Dropout                    | 0     \n",
      "21  | model.base_model.encoder.layer.0.intermediate                     | BertIntermediate           | 2 M   \n",
      "22  | model.base_model.encoder.layer.0.intermediate.dense               | Linear                     | 2 M   \n",
      "23  | model.base_model.encoder.layer.0.intermediate.intermediate_act_fn | GELUActivation             | 0     \n",
      "24  | model.base_model.encoder.layer.0.output                           | BertOutput                 | 2 M   \n",
      "25  | model.base_model.encoder.layer.0.output.dense                     | Linear                     | 2 M   \n",
      "26  | model.base_model.encoder.layer.0.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "27  | model.base_model.encoder.layer.0.output.dropout                   | Dropout                    | 0     \n",
      "28  | model.base_model.encoder.layer.1                                  | BertLayer                  | 7 M   \n",
      "29  | model.base_model.encoder.layer.1.attention                        | BertAttention              | 2 M   \n",
      "30  | model.base_model.encoder.layer.1.attention.self                   | BertSelfAttention          | 1 M   \n",
      "31  | model.base_model.encoder.layer.1.attention.self.query             | Linear                     | 590 K \n",
      "32  | model.base_model.encoder.layer.1.attention.self.key               | Linear                     | 590 K \n",
      "33  | model.base_model.encoder.layer.1.attention.self.value             | Linear                     | 590 K \n",
      "34  | model.base_model.encoder.layer.1.attention.self.dropout           | Dropout                    | 0     \n",
      "35  | model.base_model.encoder.layer.1.attention.output                 | BertSelfOutput             | 592 K \n",
      "36  | model.base_model.encoder.layer.1.attention.output.dense           | Linear                     | 590 K \n",
      "37  | model.base_model.encoder.layer.1.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "38  | model.base_model.encoder.layer.1.attention.output.dropout         | Dropout                    | 0     \n",
      "39  | model.base_model.encoder.layer.1.intermediate                     | BertIntermediate           | 2 M   \n",
      "40  | model.base_model.encoder.layer.1.intermediate.dense               | Linear                     | 2 M   \n",
      "41  | model.base_model.encoder.layer.1.output                           | BertOutput                 | 2 M   \n",
      "42  | model.base_model.encoder.layer.1.output.dense                     | Linear                     | 2 M   \n",
      "43  | model.base_model.encoder.layer.1.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "44  | model.base_model.encoder.layer.1.output.dropout                   | Dropout                    | 0     \n",
      "45  | model.base_model.encoder.layer.2                                  | BertLayer                  | 7 M   \n",
      "46  | model.base_model.encoder.layer.2.attention                        | BertAttention              | 2 M   \n",
      "47  | model.base_model.encoder.layer.2.attention.self                   | BertSelfAttention          | 1 M   \n",
      "48  | model.base_model.encoder.layer.2.attention.self.query             | Linear                     | 590 K \n",
      "49  | model.base_model.encoder.layer.2.attention.self.key               | Linear                     | 590 K \n",
      "50  | model.base_model.encoder.layer.2.attention.self.value             | Linear                     | 590 K \n",
      "51  | model.base_model.encoder.layer.2.attention.self.dropout           | Dropout                    | 0     \n",
      "52  | model.base_model.encoder.layer.2.attention.output                 | BertSelfOutput             | 592 K \n",
      "53  | model.base_model.encoder.layer.2.attention.output.dense           | Linear                     | 590 K \n",
      "54  | model.base_model.encoder.layer.2.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "55  | model.base_model.encoder.layer.2.attention.output.dropout         | Dropout                    | 0     \n",
      "56  | model.base_model.encoder.layer.2.intermediate                     | BertIntermediate           | 2 M   \n",
      "57  | model.base_model.encoder.layer.2.intermediate.dense               | Linear                     | 2 M   \n",
      "58  | model.base_model.encoder.layer.2.output                           | BertOutput                 | 2 M   \n",
      "59  | model.base_model.encoder.layer.2.output.dense                     | Linear                     | 2 M   \n",
      "60  | model.base_model.encoder.layer.2.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "61  | model.base_model.encoder.layer.2.output.dropout                   | Dropout                    | 0     \n",
      "62  | model.base_model.encoder.layer.3                                  | BertLayer                  | 7 M   \n",
      "63  | model.base_model.encoder.layer.3.attention                        | BertAttention              | 2 M   \n",
      "64  | model.base_model.encoder.layer.3.attention.self                   | BertSelfAttention          | 1 M   \n",
      "65  | model.base_model.encoder.layer.3.attention.self.query             | Linear                     | 590 K \n",
      "66  | model.base_model.encoder.layer.3.attention.self.key               | Linear                     | 590 K \n",
      "67  | model.base_model.encoder.layer.3.attention.self.value             | Linear                     | 590 K \n",
      "68  | model.base_model.encoder.layer.3.attention.self.dropout           | Dropout                    | 0     \n",
      "69  | model.base_model.encoder.layer.3.attention.output                 | BertSelfOutput             | 592 K \n",
      "70  | model.base_model.encoder.layer.3.attention.output.dense           | Linear                     | 590 K \n",
      "71  | model.base_model.encoder.layer.3.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "72  | model.base_model.encoder.layer.3.attention.output.dropout         | Dropout                    | 0     \n",
      "73  | model.base_model.encoder.layer.3.intermediate                     | BertIntermediate           | 2 M   \n",
      "74  | model.base_model.encoder.layer.3.intermediate.dense               | Linear                     | 2 M   \n",
      "75  | model.base_model.encoder.layer.3.output                           | BertOutput                 | 2 M   \n",
      "76  | model.base_model.encoder.layer.3.output.dense                     | Linear                     | 2 M   \n",
      "77  | model.base_model.encoder.layer.3.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "78  | model.base_model.encoder.layer.3.output.dropout                   | Dropout                    | 0     \n",
      "79  | model.base_model.encoder.layer.4                                  | BertLayer                  | 7 M   \n",
      "80  | model.base_model.encoder.layer.4.attention                        | BertAttention              | 2 M   \n",
      "81  | model.base_model.encoder.layer.4.attention.self                   | BertSelfAttention          | 1 M   \n",
      "82  | model.base_model.encoder.layer.4.attention.self.query             | Linear                     | 590 K \n",
      "83  | model.base_model.encoder.layer.4.attention.self.key               | Linear                     | 590 K \n",
      "84  | model.base_model.encoder.layer.4.attention.self.value             | Linear                     | 590 K \n",
      "85  | model.base_model.encoder.layer.4.attention.self.dropout           | Dropout                    | 0     \n",
      "86  | model.base_model.encoder.layer.4.attention.output                 | BertSelfOutput             | 592 K \n",
      "87  | model.base_model.encoder.layer.4.attention.output.dense           | Linear                     | 590 K \n",
      "88  | model.base_model.encoder.layer.4.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "89  | model.base_model.encoder.layer.4.attention.output.dropout         | Dropout                    | 0     \n",
      "90  | model.base_model.encoder.layer.4.intermediate                     | BertIntermediate           | 2 M   \n",
      "91  | model.base_model.encoder.layer.4.intermediate.dense               | Linear                     | 2 M   \n",
      "92  | model.base_model.encoder.layer.4.output                           | BertOutput                 | 2 M   \n",
      "93  | model.base_model.encoder.layer.4.output.dense                     | Linear                     | 2 M   \n",
      "94  | model.base_model.encoder.layer.4.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "95  | model.base_model.encoder.layer.4.output.dropout                   | Dropout                    | 0     \n",
      "96  | model.base_model.encoder.layer.5                                  | BertLayer                  | 7 M   \n",
      "97  | model.base_model.encoder.layer.5.attention                        | BertAttention              | 2 M   \n",
      "98  | model.base_model.encoder.layer.5.attention.self                   | BertSelfAttention          | 1 M   \n",
      "99  | model.base_model.encoder.layer.5.attention.self.query             | Linear                     | 590 K \n",
      "100 | model.base_model.encoder.layer.5.attention.self.key               | Linear                     | 590 K \n",
      "101 | model.base_model.encoder.layer.5.attention.self.value             | Linear                     | 590 K \n",
      "102 | model.base_model.encoder.layer.5.attention.self.dropout           | Dropout                    | 0     \n",
      "103 | model.base_model.encoder.layer.5.attention.output                 | BertSelfOutput             | 592 K \n",
      "104 | model.base_model.encoder.layer.5.attention.output.dense           | Linear                     | 590 K \n",
      "105 | model.base_model.encoder.layer.5.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "106 | model.base_model.encoder.layer.5.attention.output.dropout         | Dropout                    | 0     \n",
      "107 | model.base_model.encoder.layer.5.intermediate                     | BertIntermediate           | 2 M   \n",
      "108 | model.base_model.encoder.layer.5.intermediate.dense               | Linear                     | 2 M   \n",
      "109 | model.base_model.encoder.layer.5.output                           | BertOutput                 | 2 M   \n",
      "110 | model.base_model.encoder.layer.5.output.dense                     | Linear                     | 2 M   \n",
      "111 | model.base_model.encoder.layer.5.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "112 | model.base_model.encoder.layer.5.output.dropout                   | Dropout                    | 0     \n",
      "113 | model.base_model.encoder.layer.6                                  | BertLayer                  | 7 M   \n",
      "114 | model.base_model.encoder.layer.6.attention                        | BertAttention              | 2 M   \n",
      "115 | model.base_model.encoder.layer.6.attention.self                   | BertSelfAttention          | 1 M   \n",
      "116 | model.base_model.encoder.layer.6.attention.self.query             | Linear                     | 590 K \n",
      "117 | model.base_model.encoder.layer.6.attention.self.key               | Linear                     | 590 K \n",
      "118 | model.base_model.encoder.layer.6.attention.self.value             | Linear                     | 590 K \n",
      "119 | model.base_model.encoder.layer.6.attention.self.dropout           | Dropout                    | 0     \n",
      "120 | model.base_model.encoder.layer.6.attention.output                 | BertSelfOutput             | 592 K \n",
      "121 | model.base_model.encoder.layer.6.attention.output.dense           | Linear                     | 590 K \n",
      "122 | model.base_model.encoder.layer.6.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "123 | model.base_model.encoder.layer.6.attention.output.dropout         | Dropout                    | 0     \n",
      "124 | model.base_model.encoder.layer.6.intermediate                     | BertIntermediate           | 2 M   \n",
      "125 | model.base_model.encoder.layer.6.intermediate.dense               | Linear                     | 2 M   \n",
      "126 | model.base_model.encoder.layer.6.output                           | BertOutput                 | 2 M   \n",
      "127 | model.base_model.encoder.layer.6.output.dense                     | Linear                     | 2 M   \n",
      "128 | model.base_model.encoder.layer.6.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "129 | model.base_model.encoder.layer.6.output.dropout                   | Dropout                    | 0     \n",
      "130 | model.base_model.encoder.layer.7                                  | BertLayer                  | 7 M   \n",
      "131 | model.base_model.encoder.layer.7.attention                        | BertAttention              | 2 M   \n",
      "132 | model.base_model.encoder.layer.7.attention.self                   | BertSelfAttention          | 1 M   \n",
      "133 | model.base_model.encoder.layer.7.attention.self.query             | Linear                     | 590 K \n",
      "134 | model.base_model.encoder.layer.7.attention.self.key               | Linear                     | 590 K \n",
      "135 | model.base_model.encoder.layer.7.attention.self.value             | Linear                     | 590 K \n",
      "136 | model.base_model.encoder.layer.7.attention.self.dropout           | Dropout                    | 0     \n",
      "137 | model.base_model.encoder.layer.7.attention.output                 | BertSelfOutput             | 592 K \n",
      "138 | model.base_model.encoder.layer.7.attention.output.dense           | Linear                     | 590 K \n",
      "139 | model.base_model.encoder.layer.7.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "140 | model.base_model.encoder.layer.7.attention.output.dropout         | Dropout                    | 0     \n",
      "141 | model.base_model.encoder.layer.7.intermediate                     | BertIntermediate           | 2 M   \n",
      "142 | model.base_model.encoder.layer.7.intermediate.dense               | Linear                     | 2 M   \n",
      "143 | model.base_model.encoder.layer.7.output                           | BertOutput                 | 2 M   \n",
      "144 | model.base_model.encoder.layer.7.output.dense                     | Linear                     | 2 M   \n",
      "145 | model.base_model.encoder.layer.7.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "146 | model.base_model.encoder.layer.7.output.dropout                   | Dropout                    | 0     \n",
      "147 | model.base_model.encoder.layer.8                                  | BertLayer                  | 7 M   \n",
      "148 | model.base_model.encoder.layer.8.attention                        | BertAttention              | 2 M   \n",
      "149 | model.base_model.encoder.layer.8.attention.self                   | BertSelfAttention          | 1 M   \n",
      "150 | model.base_model.encoder.layer.8.attention.self.query             | Linear                     | 590 K \n",
      "151 | model.base_model.encoder.layer.8.attention.self.key               | Linear                     | 590 K \n",
      "152 | model.base_model.encoder.layer.8.attention.self.value             | Linear                     | 590 K \n",
      "153 | model.base_model.encoder.layer.8.attention.self.dropout           | Dropout                    | 0     \n",
      "154 | model.base_model.encoder.layer.8.attention.output                 | BertSelfOutput             | 592 K \n",
      "155 | model.base_model.encoder.layer.8.attention.output.dense           | Linear                     | 590 K \n",
      "156 | model.base_model.encoder.layer.8.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "157 | model.base_model.encoder.layer.8.attention.output.dropout         | Dropout                    | 0     \n",
      "158 | model.base_model.encoder.layer.8.intermediate                     | BertIntermediate           | 2 M   \n",
      "159 | model.base_model.encoder.layer.8.intermediate.dense               | Linear                     | 2 M   \n",
      "160 | model.base_model.encoder.layer.8.output                           | BertOutput                 | 2 M   \n",
      "161 | model.base_model.encoder.layer.8.output.dense                     | Linear                     | 2 M   \n",
      "162 | model.base_model.encoder.layer.8.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "163 | model.base_model.encoder.layer.8.output.dropout                   | Dropout                    | 0     \n",
      "164 | model.base_model.encoder.layer.9                                  | BertLayer                  | 7 M   \n",
      "165 | model.base_model.encoder.layer.9.attention                        | BertAttention              | 2 M   \n",
      "166 | model.base_model.encoder.layer.9.attention.self                   | BertSelfAttention          | 1 M   \n",
      "167 | model.base_model.encoder.layer.9.attention.self.query             | Linear                     | 590 K \n",
      "168 | model.base_model.encoder.layer.9.attention.self.key               | Linear                     | 590 K \n",
      "169 | model.base_model.encoder.layer.9.attention.self.value             | Linear                     | 590 K \n",
      "170 | model.base_model.encoder.layer.9.attention.self.dropout           | Dropout                    | 0     \n",
      "171 | model.base_model.encoder.layer.9.attention.output                 | BertSelfOutput             | 592 K \n",
      "172 | model.base_model.encoder.layer.9.attention.output.dense           | Linear                     | 590 K \n",
      "173 | model.base_model.encoder.layer.9.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "174 | model.base_model.encoder.layer.9.attention.output.dropout         | Dropout                    | 0     \n",
      "175 | model.base_model.encoder.layer.9.intermediate                     | BertIntermediate           | 2 M   \n",
      "176 | model.base_model.encoder.layer.9.intermediate.dense               | Linear                     | 2 M   \n",
      "177 | model.base_model.encoder.layer.9.output                           | BertOutput                 | 2 M   \n",
      "178 | model.base_model.encoder.layer.9.output.dense                     | Linear                     | 2 M   \n",
      "179 | model.base_model.encoder.layer.9.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "180 | model.base_model.encoder.layer.9.output.dropout                   | Dropout                    | 0     \n",
      "181 | model.base_model.encoder.layer.10                                 | BertLayer                  | 7 M   \n",
      "182 | model.base_model.encoder.layer.10.attention                       | BertAttention              | 2 M   \n",
      "183 | model.base_model.encoder.layer.10.attention.self                  | BertSelfAttention          | 1 M   \n",
      "184 | model.base_model.encoder.layer.10.attention.self.query            | Linear                     | 590 K \n",
      "185 | model.base_model.encoder.layer.10.attention.self.key              | Linear                     | 590 K \n",
      "186 | model.base_model.encoder.layer.10.attention.self.value            | Linear                     | 590 K \n",
      "187 | model.base_model.encoder.layer.10.attention.self.dropout          | Dropout                    | 0     \n",
      "188 | model.base_model.encoder.layer.10.attention.output                | BertSelfOutput             | 592 K \n",
      "189 | model.base_model.encoder.layer.10.attention.output.dense          | Linear                     | 590 K \n",
      "190 | model.base_model.encoder.layer.10.attention.output.LayerNorm      | LayerNorm                  | 1 K   \n",
      "191 | model.base_model.encoder.layer.10.attention.output.dropout        | Dropout                    | 0     \n",
      "192 | model.base_model.encoder.layer.10.intermediate                    | BertIntermediate           | 2 M   \n",
      "193 | model.base_model.encoder.layer.10.intermediate.dense              | Linear                     | 2 M   \n",
      "194 | model.base_model.encoder.layer.10.output                          | BertOutput                 | 2 M   \n",
      "195 | model.base_model.encoder.layer.10.output.dense                    | Linear                     | 2 M   \n",
      "196 | model.base_model.encoder.layer.10.output.LayerNorm                | LayerNorm                  | 1 K   \n",
      "197 | model.base_model.encoder.layer.10.output.dropout                  | Dropout                    | 0     \n",
      "198 | model.base_model.encoder.layer.11                                 | BertLayer                  | 7 M   \n",
      "199 | model.base_model.encoder.layer.11.attention                       | BertAttention              | 2 M   \n",
      "200 | model.base_model.encoder.layer.11.attention.self                  | BertSelfAttention          | 1 M   \n",
      "201 | model.base_model.encoder.layer.11.attention.self.query            | Linear                     | 590 K \n",
      "202 | model.base_model.encoder.layer.11.attention.self.key              | Linear                     | 590 K \n",
      "203 | model.base_model.encoder.layer.11.attention.self.value            | Linear                     | 590 K \n",
      "204 | model.base_model.encoder.layer.11.attention.self.dropout          | Dropout                    | 0     \n",
      "205 | model.base_model.encoder.layer.11.attention.output                | BertSelfOutput             | 592 K \n",
      "206 | model.base_model.encoder.layer.11.attention.output.dense          | Linear                     | 590 K \n",
      "207 | model.base_model.encoder.layer.11.attention.output.LayerNorm      | LayerNorm                  | 1 K   \n",
      "208 | model.base_model.encoder.layer.11.attention.output.dropout        | Dropout                    | 0     \n",
      "209 | model.base_model.encoder.layer.11.intermediate                    | BertIntermediate           | 2 M   \n",
      "210 | model.base_model.encoder.layer.11.intermediate.dense              | Linear                     | 2 M   \n",
      "211 | model.base_model.encoder.layer.11.output                          | BertOutput                 | 2 M   \n",
      "212 | model.base_model.encoder.layer.11.output.dense                    | Linear                     | 2 M   \n",
      "213 | model.base_model.encoder.layer.11.output.LayerNorm                | LayerNorm                  | 1 K   \n",
      "214 | model.base_model.encoder.layer.11.output.dropout                  | Dropout                    | 0     \n",
      "215 | model.base_model.pooler                                           | BertPooler                 | 590 K \n",
      "216 | model.base_model.pooler.dense                                     | Linear                     | 590 K \n",
      "217 | model.base_model.pooler.activation                                | Tanh                       | 0     \n",
      "218 | model.classifier                                                  | Sequential                 | 592 K \n",
      "219 | model.classifier.0                                                | Dropout                    | 0     \n",
      "220 | model.classifier.1                                                | Linear                     | 590 K \n",
      "221 | model.classifier.2                                                | Mish                       | 0     \n",
      "222 | model.classifier.3                                                | Dropout                    | 0     \n",
      "223 | model.classifier.4                                                | Linear                     | 1 K   \n",
      "224 | loss                                                              | CrossEntropyLoss           | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  87%|████████▋ | 60/69 [00:12<00:01,  4.74it/s, loss=0.198, train_loss=0.0294, v_num=113]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train (using cuda)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "                     early_stop_callback=early_stop_callback)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n",
      "________________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        weak     0.6571    0.6389    0.6479        36\n",
      "      strong     0.6389    0.6571    0.6479        35\n",
      "\n",
      "    accuracy                         0.6479        71\n",
      "   macro avg     0.6480    0.6480    0.6479        71\n",
      "weighted avg     0.6481    0.6479    0.6479        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval().cuda()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        X,y = batch_\n",
    "        input_ids = X[0]\n",
    "        attention_mask = X[1]\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        y_pred = torch.argmax(module(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(true_y, pred_y, labels=range(len(labels)))\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cm, annot=True, cmap='Greens', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# module.save_model(version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model test\n",
    "# hparams = Namespace(\n",
    "#     train_path=train_path,\n",
    "#     val_path=val_path,\n",
    "#     test_path=test_path,\n",
    "#     batch_size=10,\n",
    "#     warmup_steps=100,\n",
    "#     epochs=20,\n",
    "#     lr=2.5E-05,\n",
    "#     accumulate_grad_batches=1\n",
    "# )\n",
    "# device = torch.device('cuda:0')\n",
    "# model = TrainingModule(hparams)\n",
    "# model.model.load_state_dict(torch.load('empathy_model\\BERT_empathy_1ft_1.pt'), strict=False)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing for retrieving model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# with torch.no_grad():\n",
    "#     progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "#     model.eval().cuda()\n",
    "#     true_y, pred_y = [], []\n",
    "#     for i, batch_ in enumerate(model.test_dataloader()):\n",
    "#         X,y = batch_\n",
    "#         input_ids = X[0]\n",
    "#         attention_mask = X[1]\n",
    "#         print(progress[i % len(progress)], end=\"\\r\")\n",
    "#         y_pred = torch.argmax(model(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "#         true_y.extend(y.cpu())\n",
    "#         pred_y.extend(y_pred.cpu())\n",
    "# print(\"\\n\" + \"_\" * 80)\n",
    "# print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
