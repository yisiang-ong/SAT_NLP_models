{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import logging\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "from argparse import Namespace\n",
    "from packaging import version\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is partly adapted from the link below:\n",
    "# https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on Empathetic label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the paths for train, val, test, same split as already obtained in the T5 notebook\n",
    "train_path = \"empathy_dataset/train.txt\"\n",
    "test_path = \"empathy_dataset/test.txt\"\n",
    "val_path = \"empathy_dataset/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weak': 0, 'strong': 1}\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary associating each string label to an integer value\n",
    "\n",
    "labels = [\"weak\", \"strong\"]\n",
    "label2int = dict(zip(labels, list(range(len(labels)))))\n",
    "print(label2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpathyDataset(Dataset):\n",
    "  def __init__(\n",
    "    self,\n",
    "    path,\n",
    "    tokenizer: BertTokenizer,\n",
    "    max_token_len: int = 100\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data_column = \"text\"\n",
    "    self.class_column = \"class\"\n",
    "    self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                            engine=\"python\")\n",
    "    \n",
    "    self.max_token_len = max_token_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "    text = data_row.text\n",
    "    labels = label2int[data_row[\"class\"]]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return (encoding[\"input_ids\"].flatten(),encoding[\"attention_mask\"].flatten()), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# We are going to use bert-base-uncased\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Mish activation function \n",
    "#(from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py)\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "  \n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an EmpathyClassificationModel class to do the actual fine-tuning\n",
    "\n",
    "class EmpathyClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "        \n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, *args):\n",
    "\n",
    "        hidden_states = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch Lightning for training.\n",
    "#we use PyTorch Lighning for training. Lightning methods are defined here\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmpathyClassificationModel(BertModel.from_pretrained(\"bert-base-uncased\"), len(labels)) #was \"distilroberta-base\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.max_token_length = 128\n",
    "        self.loss = nn.CrossEntropyLoss() #cross entropy loss since this is multi-class classification\n",
    "        # self.save_hyperparameters(hparams)\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        # print(self.forward(input_ids=X[0], attention_mask = X[1]))\n",
    "        loss = self.loss(self.forward(input_ids=X[0], attention_mask = X[1]), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        \n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmpathyDataset(ds_path, self.tokenizer, self.max_token_length),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr) #we use AdamW as this usually performs well\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "   \n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), f'empathy_model/BERT_empathy_2ft.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=10,\n",
    "    warmup_steps=100,\n",
    "    epochs=30,\n",
    "    lr=2E-05,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)\n",
    "#rubbish collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # monitor validation loss\n",
    "    min_delta=0.001, #to very small change in the monitored quantity to qualify as an improvement\n",
    "    patience=20, # used to check number of time with no improvement after which training will be stopped\n",
    "    verbose=False, \n",
    "    mode=\"min\" #sed while training will stopped when the quantity monitor has stopped decreasing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:lightning:\n",
      "    | Name                                                              | Type                       | Params\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "0   | model                                                             | EmpathyClassificationModel | 110 M \n",
      "1   | model.base_model                                                  | BertModel                  | 109 M \n",
      "2   | model.base_model.embeddings                                       | BertEmbeddings             | 23 M  \n",
      "3   | model.base_model.embeddings.word_embeddings                       | Embedding                  | 23 M  \n",
      "4   | model.base_model.embeddings.position_embeddings                   | Embedding                  | 393 K \n",
      "5   | model.base_model.embeddings.token_type_embeddings                 | Embedding                  | 1 K   \n",
      "6   | model.base_model.embeddings.LayerNorm                             | LayerNorm                  | 1 K   \n",
      "7   | model.base_model.embeddings.dropout                               | Dropout                    | 0     \n",
      "8   | model.base_model.encoder                                          | BertEncoder                | 85 M  \n",
      "9   | model.base_model.encoder.layer                                    | ModuleList                 | 85 M  \n",
      "10  | model.base_model.encoder.layer.0                                  | BertLayer                  | 7 M   \n",
      "11  | model.base_model.encoder.layer.0.attention                        | BertAttention              | 2 M   \n",
      "12  | model.base_model.encoder.layer.0.attention.self                   | BertSelfAttention          | 1 M   \n",
      "13  | model.base_model.encoder.layer.0.attention.self.query             | Linear                     | 590 K \n",
      "14  | model.base_model.encoder.layer.0.attention.self.key               | Linear                     | 590 K \n",
      "15  | model.base_model.encoder.layer.0.attention.self.value             | Linear                     | 590 K \n",
      "16  | model.base_model.encoder.layer.0.attention.self.dropout           | Dropout                    | 0     \n",
      "17  | model.base_model.encoder.layer.0.attention.output                 | BertSelfOutput             | 592 K \n",
      "18  | model.base_model.encoder.layer.0.attention.output.dense           | Linear                     | 590 K \n",
      "19  | model.base_model.encoder.layer.0.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "20  | model.base_model.encoder.layer.0.attention.output.dropout         | Dropout                    | 0     \n",
      "21  | model.base_model.encoder.layer.0.intermediate                     | BertIntermediate           | 2 M   \n",
      "22  | model.base_model.encoder.layer.0.intermediate.dense               | Linear                     | 2 M   \n",
      "23  | model.base_model.encoder.layer.0.intermediate.intermediate_act_fn | GELUActivation             | 0     \n",
      "24  | model.base_model.encoder.layer.0.output                           | BertOutput                 | 2 M   \n",
      "25  | model.base_model.encoder.layer.0.output.dense                     | Linear                     | 2 M   \n",
      "26  | model.base_model.encoder.layer.0.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "27  | model.base_model.encoder.layer.0.output.dropout                   | Dropout                    | 0     \n",
      "28  | model.base_model.encoder.layer.1                                  | BertLayer                  | 7 M   \n",
      "29  | model.base_model.encoder.layer.1.attention                        | BertAttention              | 2 M   \n",
      "30  | model.base_model.encoder.layer.1.attention.self                   | BertSelfAttention          | 1 M   \n",
      "31  | model.base_model.encoder.layer.1.attention.self.query             | Linear                     | 590 K \n",
      "32  | model.base_model.encoder.layer.1.attention.self.key               | Linear                     | 590 K \n",
      "33  | model.base_model.encoder.layer.1.attention.self.value             | Linear                     | 590 K \n",
      "34  | model.base_model.encoder.layer.1.attention.self.dropout           | Dropout                    | 0     \n",
      "35  | model.base_model.encoder.layer.1.attention.output                 | BertSelfOutput             | 592 K \n",
      "36  | model.base_model.encoder.layer.1.attention.output.dense           | Linear                     | 590 K \n",
      "37  | model.base_model.encoder.layer.1.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "38  | model.base_model.encoder.layer.1.attention.output.dropout         | Dropout                    | 0     \n",
      "39  | model.base_model.encoder.layer.1.intermediate                     | BertIntermediate           | 2 M   \n",
      "40  | model.base_model.encoder.layer.1.intermediate.dense               | Linear                     | 2 M   \n",
      "41  | model.base_model.encoder.layer.1.output                           | BertOutput                 | 2 M   \n",
      "42  | model.base_model.encoder.layer.1.output.dense                     | Linear                     | 2 M   \n",
      "43  | model.base_model.encoder.layer.1.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "44  | model.base_model.encoder.layer.1.output.dropout                   | Dropout                    | 0     \n",
      "45  | model.base_model.encoder.layer.2                                  | BertLayer                  | 7 M   \n",
      "46  | model.base_model.encoder.layer.2.attention                        | BertAttention              | 2 M   \n",
      "47  | model.base_model.encoder.layer.2.attention.self                   | BertSelfAttention          | 1 M   \n",
      "48  | model.base_model.encoder.layer.2.attention.self.query             | Linear                     | 590 K \n",
      "49  | model.base_model.encoder.layer.2.attention.self.key               | Linear                     | 590 K \n",
      "50  | model.base_model.encoder.layer.2.attention.self.value             | Linear                     | 590 K \n",
      "51  | model.base_model.encoder.layer.2.attention.self.dropout           | Dropout                    | 0     \n",
      "52  | model.base_model.encoder.layer.2.attention.output                 | BertSelfOutput             | 592 K \n",
      "53  | model.base_model.encoder.layer.2.attention.output.dense           | Linear                     | 590 K \n",
      "54  | model.base_model.encoder.layer.2.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "55  | model.base_model.encoder.layer.2.attention.output.dropout         | Dropout                    | 0     \n",
      "56  | model.base_model.encoder.layer.2.intermediate                     | BertIntermediate           | 2 M   \n",
      "57  | model.base_model.encoder.layer.2.intermediate.dense               | Linear                     | 2 M   \n",
      "58  | model.base_model.encoder.layer.2.output                           | BertOutput                 | 2 M   \n",
      "59  | model.base_model.encoder.layer.2.output.dense                     | Linear                     | 2 M   \n",
      "60  | model.base_model.encoder.layer.2.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "61  | model.base_model.encoder.layer.2.output.dropout                   | Dropout                    | 0     \n",
      "62  | model.base_model.encoder.layer.3                                  | BertLayer                  | 7 M   \n",
      "63  | model.base_model.encoder.layer.3.attention                        | BertAttention              | 2 M   \n",
      "64  | model.base_model.encoder.layer.3.attention.self                   | BertSelfAttention          | 1 M   \n",
      "65  | model.base_model.encoder.layer.3.attention.self.query             | Linear                     | 590 K \n",
      "66  | model.base_model.encoder.layer.3.attention.self.key               | Linear                     | 590 K \n",
      "67  | model.base_model.encoder.layer.3.attention.self.value             | Linear                     | 590 K \n",
      "68  | model.base_model.encoder.layer.3.attention.self.dropout           | Dropout                    | 0     \n",
      "69  | model.base_model.encoder.layer.3.attention.output                 | BertSelfOutput             | 592 K \n",
      "70  | model.base_model.encoder.layer.3.attention.output.dense           | Linear                     | 590 K \n",
      "71  | model.base_model.encoder.layer.3.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "72  | model.base_model.encoder.layer.3.attention.output.dropout         | Dropout                    | 0     \n",
      "73  | model.base_model.encoder.layer.3.intermediate                     | BertIntermediate           | 2 M   \n",
      "74  | model.base_model.encoder.layer.3.intermediate.dense               | Linear                     | 2 M   \n",
      "75  | model.base_model.encoder.layer.3.output                           | BertOutput                 | 2 M   \n",
      "76  | model.base_model.encoder.layer.3.output.dense                     | Linear                     | 2 M   \n",
      "77  | model.base_model.encoder.layer.3.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "78  | model.base_model.encoder.layer.3.output.dropout                   | Dropout                    | 0     \n",
      "79  | model.base_model.encoder.layer.4                                  | BertLayer                  | 7 M   \n",
      "80  | model.base_model.encoder.layer.4.attention                        | BertAttention              | 2 M   \n",
      "81  | model.base_model.encoder.layer.4.attention.self                   | BertSelfAttention          | 1 M   \n",
      "82  | model.base_model.encoder.layer.4.attention.self.query             | Linear                     | 590 K \n",
      "83  | model.base_model.encoder.layer.4.attention.self.key               | Linear                     | 590 K \n",
      "84  | model.base_model.encoder.layer.4.attention.self.value             | Linear                     | 590 K \n",
      "85  | model.base_model.encoder.layer.4.attention.self.dropout           | Dropout                    | 0     \n",
      "86  | model.base_model.encoder.layer.4.attention.output                 | BertSelfOutput             | 592 K \n",
      "87  | model.base_model.encoder.layer.4.attention.output.dense           | Linear                     | 590 K \n",
      "88  | model.base_model.encoder.layer.4.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "89  | model.base_model.encoder.layer.4.attention.output.dropout         | Dropout                    | 0     \n",
      "90  | model.base_model.encoder.layer.4.intermediate                     | BertIntermediate           | 2 M   \n",
      "91  | model.base_model.encoder.layer.4.intermediate.dense               | Linear                     | 2 M   \n",
      "92  | model.base_model.encoder.layer.4.output                           | BertOutput                 | 2 M   \n",
      "93  | model.base_model.encoder.layer.4.output.dense                     | Linear                     | 2 M   \n",
      "94  | model.base_model.encoder.layer.4.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "95  | model.base_model.encoder.layer.4.output.dropout                   | Dropout                    | 0     \n",
      "96  | model.base_model.encoder.layer.5                                  | BertLayer                  | 7 M   \n",
      "97  | model.base_model.encoder.layer.5.attention                        | BertAttention              | 2 M   \n",
      "98  | model.base_model.encoder.layer.5.attention.self                   | BertSelfAttention          | 1 M   \n",
      "99  | model.base_model.encoder.layer.5.attention.self.query             | Linear                     | 590 K \n",
      "100 | model.base_model.encoder.layer.5.attention.self.key               | Linear                     | 590 K \n",
      "101 | model.base_model.encoder.layer.5.attention.self.value             | Linear                     | 590 K \n",
      "102 | model.base_model.encoder.layer.5.attention.self.dropout           | Dropout                    | 0     \n",
      "103 | model.base_model.encoder.layer.5.attention.output                 | BertSelfOutput             | 592 K \n",
      "104 | model.base_model.encoder.layer.5.attention.output.dense           | Linear                     | 590 K \n",
      "105 | model.base_model.encoder.layer.5.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "106 | model.base_model.encoder.layer.5.attention.output.dropout         | Dropout                    | 0     \n",
      "107 | model.base_model.encoder.layer.5.intermediate                     | BertIntermediate           | 2 M   \n",
      "108 | model.base_model.encoder.layer.5.intermediate.dense               | Linear                     | 2 M   \n",
      "109 | model.base_model.encoder.layer.5.output                           | BertOutput                 | 2 M   \n",
      "110 | model.base_model.encoder.layer.5.output.dense                     | Linear                     | 2 M   \n",
      "111 | model.base_model.encoder.layer.5.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "112 | model.base_model.encoder.layer.5.output.dropout                   | Dropout                    | 0     \n",
      "113 | model.base_model.encoder.layer.6                                  | BertLayer                  | 7 M   \n",
      "114 | model.base_model.encoder.layer.6.attention                        | BertAttention              | 2 M   \n",
      "115 | model.base_model.encoder.layer.6.attention.self                   | BertSelfAttention          | 1 M   \n",
      "116 | model.base_model.encoder.layer.6.attention.self.query             | Linear                     | 590 K \n",
      "117 | model.base_model.encoder.layer.6.attention.self.key               | Linear                     | 590 K \n",
      "118 | model.base_model.encoder.layer.6.attention.self.value             | Linear                     | 590 K \n",
      "119 | model.base_model.encoder.layer.6.attention.self.dropout           | Dropout                    | 0     \n",
      "120 | model.base_model.encoder.layer.6.attention.output                 | BertSelfOutput             | 592 K \n",
      "121 | model.base_model.encoder.layer.6.attention.output.dense           | Linear                     | 590 K \n",
      "122 | model.base_model.encoder.layer.6.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "123 | model.base_model.encoder.layer.6.attention.output.dropout         | Dropout                    | 0     \n",
      "124 | model.base_model.encoder.layer.6.intermediate                     | BertIntermediate           | 2 M   \n",
      "125 | model.base_model.encoder.layer.6.intermediate.dense               | Linear                     | 2 M   \n",
      "126 | model.base_model.encoder.layer.6.output                           | BertOutput                 | 2 M   \n",
      "127 | model.base_model.encoder.layer.6.output.dense                     | Linear                     | 2 M   \n",
      "128 | model.base_model.encoder.layer.6.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "129 | model.base_model.encoder.layer.6.output.dropout                   | Dropout                    | 0     \n",
      "130 | model.base_model.encoder.layer.7                                  | BertLayer                  | 7 M   \n",
      "131 | model.base_model.encoder.layer.7.attention                        | BertAttention              | 2 M   \n",
      "132 | model.base_model.encoder.layer.7.attention.self                   | BertSelfAttention          | 1 M   \n",
      "133 | model.base_model.encoder.layer.7.attention.self.query             | Linear                     | 590 K \n",
      "134 | model.base_model.encoder.layer.7.attention.self.key               | Linear                     | 590 K \n",
      "135 | model.base_model.encoder.layer.7.attention.self.value             | Linear                     | 590 K \n",
      "136 | model.base_model.encoder.layer.7.attention.self.dropout           | Dropout                    | 0     \n",
      "137 | model.base_model.encoder.layer.7.attention.output                 | BertSelfOutput             | 592 K \n",
      "138 | model.base_model.encoder.layer.7.attention.output.dense           | Linear                     | 590 K \n",
      "139 | model.base_model.encoder.layer.7.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "140 | model.base_model.encoder.layer.7.attention.output.dropout         | Dropout                    | 0     \n",
      "141 | model.base_model.encoder.layer.7.intermediate                     | BertIntermediate           | 2 M   \n",
      "142 | model.base_model.encoder.layer.7.intermediate.dense               | Linear                     | 2 M   \n",
      "143 | model.base_model.encoder.layer.7.output                           | BertOutput                 | 2 M   \n",
      "144 | model.base_model.encoder.layer.7.output.dense                     | Linear                     | 2 M   \n",
      "145 | model.base_model.encoder.layer.7.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "146 | model.base_model.encoder.layer.7.output.dropout                   | Dropout                    | 0     \n",
      "147 | model.base_model.encoder.layer.8                                  | BertLayer                  | 7 M   \n",
      "148 | model.base_model.encoder.layer.8.attention                        | BertAttention              | 2 M   \n",
      "149 | model.base_model.encoder.layer.8.attention.self                   | BertSelfAttention          | 1 M   \n",
      "150 | model.base_model.encoder.layer.8.attention.self.query             | Linear                     | 590 K \n",
      "151 | model.base_model.encoder.layer.8.attention.self.key               | Linear                     | 590 K \n",
      "152 | model.base_model.encoder.layer.8.attention.self.value             | Linear                     | 590 K \n",
      "153 | model.base_model.encoder.layer.8.attention.self.dropout           | Dropout                    | 0     \n",
      "154 | model.base_model.encoder.layer.8.attention.output                 | BertSelfOutput             | 592 K \n",
      "155 | model.base_model.encoder.layer.8.attention.output.dense           | Linear                     | 590 K \n",
      "156 | model.base_model.encoder.layer.8.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "157 | model.base_model.encoder.layer.8.attention.output.dropout         | Dropout                    | 0     \n",
      "158 | model.base_model.encoder.layer.8.intermediate                     | BertIntermediate           | 2 M   \n",
      "159 | model.base_model.encoder.layer.8.intermediate.dense               | Linear                     | 2 M   \n",
      "160 | model.base_model.encoder.layer.8.output                           | BertOutput                 | 2 M   \n",
      "161 | model.base_model.encoder.layer.8.output.dense                     | Linear                     | 2 M   \n",
      "162 | model.base_model.encoder.layer.8.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "163 | model.base_model.encoder.layer.8.output.dropout                   | Dropout                    | 0     \n",
      "164 | model.base_model.encoder.layer.9                                  | BertLayer                  | 7 M   \n",
      "165 | model.base_model.encoder.layer.9.attention                        | BertAttention              | 2 M   \n",
      "166 | model.base_model.encoder.layer.9.attention.self                   | BertSelfAttention          | 1 M   \n",
      "167 | model.base_model.encoder.layer.9.attention.self.query             | Linear                     | 590 K \n",
      "168 | model.base_model.encoder.layer.9.attention.self.key               | Linear                     | 590 K \n",
      "169 | model.base_model.encoder.layer.9.attention.self.value             | Linear                     | 590 K \n",
      "170 | model.base_model.encoder.layer.9.attention.self.dropout           | Dropout                    | 0     \n",
      "171 | model.base_model.encoder.layer.9.attention.output                 | BertSelfOutput             | 592 K \n",
      "172 | model.base_model.encoder.layer.9.attention.output.dense           | Linear                     | 590 K \n",
      "173 | model.base_model.encoder.layer.9.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "174 | model.base_model.encoder.layer.9.attention.output.dropout         | Dropout                    | 0     \n",
      "175 | model.base_model.encoder.layer.9.intermediate                     | BertIntermediate           | 2 M   \n",
      "176 | model.base_model.encoder.layer.9.intermediate.dense               | Linear                     | 2 M   \n",
      "177 | model.base_model.encoder.layer.9.output                           | BertOutput                 | 2 M   \n",
      "178 | model.base_model.encoder.layer.9.output.dense                     | Linear                     | 2 M   \n",
      "179 | model.base_model.encoder.layer.9.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "180 | model.base_model.encoder.layer.9.output.dropout                   | Dropout                    | 0     \n",
      "181 | model.base_model.encoder.layer.10                                 | BertLayer                  | 7 M   \n",
      "182 | model.base_model.encoder.layer.10.attention                       | BertAttention              | 2 M   \n",
      "183 | model.base_model.encoder.layer.10.attention.self                  | BertSelfAttention          | 1 M   \n",
      "184 | model.base_model.encoder.layer.10.attention.self.query            | Linear                     | 590 K \n",
      "185 | model.base_model.encoder.layer.10.attention.self.key              | Linear                     | 590 K \n",
      "186 | model.base_model.encoder.layer.10.attention.self.value            | Linear                     | 590 K \n",
      "187 | model.base_model.encoder.layer.10.attention.self.dropout          | Dropout                    | 0     \n",
      "188 | model.base_model.encoder.layer.10.attention.output                | BertSelfOutput             | 592 K \n",
      "189 | model.base_model.encoder.layer.10.attention.output.dense          | Linear                     | 590 K \n",
      "190 | model.base_model.encoder.layer.10.attention.output.LayerNorm      | LayerNorm                  | 1 K   \n",
      "191 | model.base_model.encoder.layer.10.attention.output.dropout        | Dropout                    | 0     \n",
      "192 | model.base_model.encoder.layer.10.intermediate                    | BertIntermediate           | 2 M   \n",
      "193 | model.base_model.encoder.layer.10.intermediate.dense              | Linear                     | 2 M   \n",
      "194 | model.base_model.encoder.layer.10.output                          | BertOutput                 | 2 M   \n",
      "195 | model.base_model.encoder.layer.10.output.dense                    | Linear                     | 2 M   \n",
      "196 | model.base_model.encoder.layer.10.output.LayerNorm                | LayerNorm                  | 1 K   \n",
      "197 | model.base_model.encoder.layer.10.output.dropout                  | Dropout                    | 0     \n",
      "198 | model.base_model.encoder.layer.11                                 | BertLayer                  | 7 M   \n",
      "199 | model.base_model.encoder.layer.11.attention                       | BertAttention              | 2 M   \n",
      "200 | model.base_model.encoder.layer.11.attention.self                  | BertSelfAttention          | 1 M   \n",
      "201 | model.base_model.encoder.layer.11.attention.self.query            | Linear                     | 590 K \n",
      "202 | model.base_model.encoder.layer.11.attention.self.key              | Linear                     | 590 K \n",
      "203 | model.base_model.encoder.layer.11.attention.self.value            | Linear                     | 590 K \n",
      "204 | model.base_model.encoder.layer.11.attention.self.dropout          | Dropout                    | 0     \n",
      "205 | model.base_model.encoder.layer.11.attention.output                | BertSelfOutput             | 592 K \n",
      "206 | model.base_model.encoder.layer.11.attention.output.dense          | Linear                     | 590 K \n",
      "207 | model.base_model.encoder.layer.11.attention.output.LayerNorm      | LayerNorm                  | 1 K   \n",
      "208 | model.base_model.encoder.layer.11.attention.output.dropout        | Dropout                    | 0     \n",
      "209 | model.base_model.encoder.layer.11.intermediate                    | BertIntermediate           | 2 M   \n",
      "210 | model.base_model.encoder.layer.11.intermediate.dense              | Linear                     | 2 M   \n",
      "211 | model.base_model.encoder.layer.11.output                          | BertOutput                 | 2 M   \n",
      "212 | model.base_model.encoder.layer.11.output.dense                    | Linear                     | 2 M   \n",
      "213 | model.base_model.encoder.layer.11.output.LayerNorm                | LayerNorm                  | 1 K   \n",
      "214 | model.base_model.encoder.layer.11.output.dropout                  | Dropout                    | 0     \n",
      "215 | model.base_model.pooler                                           | BertPooler                 | 590 K \n",
      "216 | model.base_model.pooler.dense                                     | Linear                     | 590 K \n",
      "217 | model.base_model.pooler.activation                                | Tanh                       | 0     \n",
      "218 | model.classifier                                                  | Sequential                 | 592 K \n",
      "219 | model.classifier.0                                                | Dropout                    | 0     \n",
      "220 | model.classifier.1                                                | Linear                     | 590 K \n",
      "221 | model.classifier.2                                                | Mish                       | 0     \n",
      "222 | model.classifier.3                                                | Dropout                    | 0     \n",
      "223 | model.classifier.4                                                | Linear                     | 1 K   \n",
      "224 | loss                                                              | CrossEntropyLoss           | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  81%|████████  | 60/74 [00:09<00:02,  6.34it/s, loss=0.011, train_loss=0.00121, v_num=117]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train (using cuda)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "                     early_stop_callback=early_stop_callback)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# module.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingModule(\n",
       "  (model): EmpathyClassificationModel(\n",
       "    (base_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.05, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (2): Mish()\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model test\n",
    "# hparams = Namespace(\n",
    "#     train_path=train_path,\n",
    "#     val_path=val_path,\n",
    "#     test_path=test_path,\n",
    "#     batch_size=10,\n",
    "#     warmup_steps=100,\n",
    "#     epochs=20,\n",
    "#     lr=2.5E-05,\n",
    "#     accumulate_grad_batches=1\n",
    "# )\n",
    "# device = torch.device('cuda:0')\n",
    "# module = TrainingModule(hparams)\n",
    "# module.model.load_state_dict(torch.load('empathy_model/BERT_empathy_2ft.pt'), strict=False)\n",
    "# module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n",
      "________________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        weak     0.7917    0.8261    0.8085        23\n",
      "      strong     0.7333    0.6875    0.7097        16\n",
      "\n",
      "    accuracy                         0.7692        39\n",
      "   macro avg     0.7625    0.7568    0.7591        39\n",
      "weighted avg     0.7677    0.7692    0.7680        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval().cuda()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        X,y = batch_\n",
    "        input_ids = X[0]\n",
    "        attention_mask = X[1]\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        y_pred = torch.argmax(module(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on EmpatheticPersonas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the paths for train, val, test\n",
    "train_path = \"empathy_dataset/my_train.txt\"\n",
    "test_path = \"empathy_dataset/my_test.txt\"\n",
    "val_path = \"empathy_dataset/my_val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch Lightning for training.\n",
    "#we use PyTorch Lighning for training. Lightning methods are defined here\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmpathyClassificationModel(BertModel.from_pretrained(\"bert-base-uncased\"), len(labels)) #was \"distilroberta-base\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.max_token_length = 128\n",
    "        self.loss = nn.CrossEntropyLoss() #cross entropy loss since this is multi-class classification\n",
    "        # self.save_hyperparameters(hparams)\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        # print(self.forward(input_ids=X[0], attention_mask = X[1]))\n",
    "        loss = self.loss(self.forward(input_ids=X[0], attention_mask = X[1]), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        \n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmpathyDataset(ds_path, self.tokenizer, self.max_token_length),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr) #we use AdamW as this usually performs well\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "   \n",
    "    def save_model(self, version):\n",
    "        torch.save(self.model.state_dict(), f'empathy_model/BERT_empathy_2ft_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=10,\n",
    "    warmup_steps=100,\n",
    "    epochs=20,\n",
    "    lr=5E-06,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "#rubbish collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingModule(\n",
       "  (model): EmpathyClassificationModel(\n",
       "    (base_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.05, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (2): Mish()\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "module = TrainingModule(hparams)\n",
    "module.model.load_state_dict(torch.load('empathy_model/BERT_empathy_2ft.pt'), strict=False)\n",
    "module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # monitor validation loss\n",
    "    min_delta=0.001, #to very small change in the monitored quantity to qualify as an improvement\n",
    "    patience=20, # used to check number of time with no improvement after which training will be stopped\n",
    "    verbose=False, \n",
    "    mode=\"min\" #sed while training will stopped when the quantity monitor has stopped decreasing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:lightning:\n",
      "    | Name                                                              | Type                       | Params\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "0   | model                                                             | EmpathyClassificationModel | 110 M \n",
      "1   | model.base_model                                                  | BertModel                  | 109 M \n",
      "2   | model.base_model.embeddings                                       | BertEmbeddings             | 23 M  \n",
      "3   | model.base_model.embeddings.word_embeddings                       | Embedding                  | 23 M  \n",
      "4   | model.base_model.embeddings.position_embeddings                   | Embedding                  | 393 K \n",
      "5   | model.base_model.embeddings.token_type_embeddings                 | Embedding                  | 1 K   \n",
      "6   | model.base_model.embeddings.LayerNorm                             | LayerNorm                  | 1 K   \n",
      "7   | model.base_model.embeddings.dropout                               | Dropout                    | 0     \n",
      "8   | model.base_model.encoder                                          | BertEncoder                | 85 M  \n",
      "9   | model.base_model.encoder.layer                                    | ModuleList                 | 85 M  \n",
      "10  | model.base_model.encoder.layer.0                                  | BertLayer                  | 7 M   \n",
      "11  | model.base_model.encoder.layer.0.attention                        | BertAttention              | 2 M   \n",
      "12  | model.base_model.encoder.layer.0.attention.self                   | BertSelfAttention          | 1 M   \n",
      "13  | model.base_model.encoder.layer.0.attention.self.query             | Linear                     | 590 K \n",
      "14  | model.base_model.encoder.layer.0.attention.self.key               | Linear                     | 590 K \n",
      "15  | model.base_model.encoder.layer.0.attention.self.value             | Linear                     | 590 K \n",
      "16  | model.base_model.encoder.layer.0.attention.self.dropout           | Dropout                    | 0     \n",
      "17  | model.base_model.encoder.layer.0.attention.output                 | BertSelfOutput             | 592 K \n",
      "18  | model.base_model.encoder.layer.0.attention.output.dense           | Linear                     | 590 K \n",
      "19  | model.base_model.encoder.layer.0.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "20  | model.base_model.encoder.layer.0.attention.output.dropout         | Dropout                    | 0     \n",
      "21  | model.base_model.encoder.layer.0.intermediate                     | BertIntermediate           | 2 M   \n",
      "22  | model.base_model.encoder.layer.0.intermediate.dense               | Linear                     | 2 M   \n",
      "23  | model.base_model.encoder.layer.0.intermediate.intermediate_act_fn | GELUActivation             | 0     \n",
      "24  | model.base_model.encoder.layer.0.output                           | BertOutput                 | 2 M   \n",
      "25  | model.base_model.encoder.layer.0.output.dense                     | Linear                     | 2 M   \n",
      "26  | model.base_model.encoder.layer.0.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "27  | model.base_model.encoder.layer.0.output.dropout                   | Dropout                    | 0     \n",
      "28  | model.base_model.encoder.layer.1                                  | BertLayer                  | 7 M   \n",
      "29  | model.base_model.encoder.layer.1.attention                        | BertAttention              | 2 M   \n",
      "30  | model.base_model.encoder.layer.1.attention.self                   | BertSelfAttention          | 1 M   \n",
      "31  | model.base_model.encoder.layer.1.attention.self.query             | Linear                     | 590 K \n",
      "32  | model.base_model.encoder.layer.1.attention.self.key               | Linear                     | 590 K \n",
      "33  | model.base_model.encoder.layer.1.attention.self.value             | Linear                     | 590 K \n",
      "34  | model.base_model.encoder.layer.1.attention.self.dropout           | Dropout                    | 0     \n",
      "35  | model.base_model.encoder.layer.1.attention.output                 | BertSelfOutput             | 592 K \n",
      "36  | model.base_model.encoder.layer.1.attention.output.dense           | Linear                     | 590 K \n",
      "37  | model.base_model.encoder.layer.1.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "38  | model.base_model.encoder.layer.1.attention.output.dropout         | Dropout                    | 0     \n",
      "39  | model.base_model.encoder.layer.1.intermediate                     | BertIntermediate           | 2 M   \n",
      "40  | model.base_model.encoder.layer.1.intermediate.dense               | Linear                     | 2 M   \n",
      "41  | model.base_model.encoder.layer.1.output                           | BertOutput                 | 2 M   \n",
      "42  | model.base_model.encoder.layer.1.output.dense                     | Linear                     | 2 M   \n",
      "43  | model.base_model.encoder.layer.1.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "44  | model.base_model.encoder.layer.1.output.dropout                   | Dropout                    | 0     \n",
      "45  | model.base_model.encoder.layer.2                                  | BertLayer                  | 7 M   \n",
      "46  | model.base_model.encoder.layer.2.attention                        | BertAttention              | 2 M   \n",
      "47  | model.base_model.encoder.layer.2.attention.self                   | BertSelfAttention          | 1 M   \n",
      "48  | model.base_model.encoder.layer.2.attention.self.query             | Linear                     | 590 K \n",
      "49  | model.base_model.encoder.layer.2.attention.self.key               | Linear                     | 590 K \n",
      "50  | model.base_model.encoder.layer.2.attention.self.value             | Linear                     | 590 K \n",
      "51  | model.base_model.encoder.layer.2.attention.self.dropout           | Dropout                    | 0     \n",
      "52  | model.base_model.encoder.layer.2.attention.output                 | BertSelfOutput             | 592 K \n",
      "53  | model.base_model.encoder.layer.2.attention.output.dense           | Linear                     | 590 K \n",
      "54  | model.base_model.encoder.layer.2.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "55  | model.base_model.encoder.layer.2.attention.output.dropout         | Dropout                    | 0     \n",
      "56  | model.base_model.encoder.layer.2.intermediate                     | BertIntermediate           | 2 M   \n",
      "57  | model.base_model.encoder.layer.2.intermediate.dense               | Linear                     | 2 M   \n",
      "58  | model.base_model.encoder.layer.2.output                           | BertOutput                 | 2 M   \n",
      "59  | model.base_model.encoder.layer.2.output.dense                     | Linear                     | 2 M   \n",
      "60  | model.base_model.encoder.layer.2.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "61  | model.base_model.encoder.layer.2.output.dropout                   | Dropout                    | 0     \n",
      "62  | model.base_model.encoder.layer.3                                  | BertLayer                  | 7 M   \n",
      "63  | model.base_model.encoder.layer.3.attention                        | BertAttention              | 2 M   \n",
      "64  | model.base_model.encoder.layer.3.attention.self                   | BertSelfAttention          | 1 M   \n",
      "65  | model.base_model.encoder.layer.3.attention.self.query             | Linear                     | 590 K \n",
      "66  | model.base_model.encoder.layer.3.attention.self.key               | Linear                     | 590 K \n",
      "67  | model.base_model.encoder.layer.3.attention.self.value             | Linear                     | 590 K \n",
      "68  | model.base_model.encoder.layer.3.attention.self.dropout           | Dropout                    | 0     \n",
      "69  | model.base_model.encoder.layer.3.attention.output                 | BertSelfOutput             | 592 K \n",
      "70  | model.base_model.encoder.layer.3.attention.output.dense           | Linear                     | 590 K \n",
      "71  | model.base_model.encoder.layer.3.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "72  | model.base_model.encoder.layer.3.attention.output.dropout         | Dropout                    | 0     \n",
      "73  | model.base_model.encoder.layer.3.intermediate                     | BertIntermediate           | 2 M   \n",
      "74  | model.base_model.encoder.layer.3.intermediate.dense               | Linear                     | 2 M   \n",
      "75  | model.base_model.encoder.layer.3.output                           | BertOutput                 | 2 M   \n",
      "76  | model.base_model.encoder.layer.3.output.dense                     | Linear                     | 2 M   \n",
      "77  | model.base_model.encoder.layer.3.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "78  | model.base_model.encoder.layer.3.output.dropout                   | Dropout                    | 0     \n",
      "79  | model.base_model.encoder.layer.4                                  | BertLayer                  | 7 M   \n",
      "80  | model.base_model.encoder.layer.4.attention                        | BertAttention              | 2 M   \n",
      "81  | model.base_model.encoder.layer.4.attention.self                   | BertSelfAttention          | 1 M   \n",
      "82  | model.base_model.encoder.layer.4.attention.self.query             | Linear                     | 590 K \n",
      "83  | model.base_model.encoder.layer.4.attention.self.key               | Linear                     | 590 K \n",
      "84  | model.base_model.encoder.layer.4.attention.self.value             | Linear                     | 590 K \n",
      "85  | model.base_model.encoder.layer.4.attention.self.dropout           | Dropout                    | 0     \n",
      "86  | model.base_model.encoder.layer.4.attention.output                 | BertSelfOutput             | 592 K \n",
      "87  | model.base_model.encoder.layer.4.attention.output.dense           | Linear                     | 590 K \n",
      "88  | model.base_model.encoder.layer.4.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "89  | model.base_model.encoder.layer.4.attention.output.dropout         | Dropout                    | 0     \n",
      "90  | model.base_model.encoder.layer.4.intermediate                     | BertIntermediate           | 2 M   \n",
      "91  | model.base_model.encoder.layer.4.intermediate.dense               | Linear                     | 2 M   \n",
      "92  | model.base_model.encoder.layer.4.output                           | BertOutput                 | 2 M   \n",
      "93  | model.base_model.encoder.layer.4.output.dense                     | Linear                     | 2 M   \n",
      "94  | model.base_model.encoder.layer.4.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "95  | model.base_model.encoder.layer.4.output.dropout                   | Dropout                    | 0     \n",
      "96  | model.base_model.encoder.layer.5                                  | BertLayer                  | 7 M   \n",
      "97  | model.base_model.encoder.layer.5.attention                        | BertAttention              | 2 M   \n",
      "98  | model.base_model.encoder.layer.5.attention.self                   | BertSelfAttention          | 1 M   \n",
      "99  | model.base_model.encoder.layer.5.attention.self.query             | Linear                     | 590 K \n",
      "100 | model.base_model.encoder.layer.5.attention.self.key               | Linear                     | 590 K \n",
      "101 | model.base_model.encoder.layer.5.attention.self.value             | Linear                     | 590 K \n",
      "102 | model.base_model.encoder.layer.5.attention.self.dropout           | Dropout                    | 0     \n",
      "103 | model.base_model.encoder.layer.5.attention.output                 | BertSelfOutput             | 592 K \n",
      "104 | model.base_model.encoder.layer.5.attention.output.dense           | Linear                     | 590 K \n",
      "105 | model.base_model.encoder.layer.5.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "106 | model.base_model.encoder.layer.5.attention.output.dropout         | Dropout                    | 0     \n",
      "107 | model.base_model.encoder.layer.5.intermediate                     | BertIntermediate           | 2 M   \n",
      "108 | model.base_model.encoder.layer.5.intermediate.dense               | Linear                     | 2 M   \n",
      "109 | model.base_model.encoder.layer.5.output                           | BertOutput                 | 2 M   \n",
      "110 | model.base_model.encoder.layer.5.output.dense                     | Linear                     | 2 M   \n",
      "111 | model.base_model.encoder.layer.5.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "112 | model.base_model.encoder.layer.5.output.dropout                   | Dropout                    | 0     \n",
      "113 | model.base_model.encoder.layer.6                                  | BertLayer                  | 7 M   \n",
      "114 | model.base_model.encoder.layer.6.attention                        | BertAttention              | 2 M   \n",
      "115 | model.base_model.encoder.layer.6.attention.self                   | BertSelfAttention          | 1 M   \n",
      "116 | model.base_model.encoder.layer.6.attention.self.query             | Linear                     | 590 K \n",
      "117 | model.base_model.encoder.layer.6.attention.self.key               | Linear                     | 590 K \n",
      "118 | model.base_model.encoder.layer.6.attention.self.value             | Linear                     | 590 K \n",
      "119 | model.base_model.encoder.layer.6.attention.self.dropout           | Dropout                    | 0     \n",
      "120 | model.base_model.encoder.layer.6.attention.output                 | BertSelfOutput             | 592 K \n",
      "121 | model.base_model.encoder.layer.6.attention.output.dense           | Linear                     | 590 K \n",
      "122 | model.base_model.encoder.layer.6.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "123 | model.base_model.encoder.layer.6.attention.output.dropout         | Dropout                    | 0     \n",
      "124 | model.base_model.encoder.layer.6.intermediate                     | BertIntermediate           | 2 M   \n",
      "125 | model.base_model.encoder.layer.6.intermediate.dense               | Linear                     | 2 M   \n",
      "126 | model.base_model.encoder.layer.6.output                           | BertOutput                 | 2 M   \n",
      "127 | model.base_model.encoder.layer.6.output.dense                     | Linear                     | 2 M   \n",
      "128 | model.base_model.encoder.layer.6.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "129 | model.base_model.encoder.layer.6.output.dropout                   | Dropout                    | 0     \n",
      "130 | model.base_model.encoder.layer.7                                  | BertLayer                  | 7 M   \n",
      "131 | model.base_model.encoder.layer.7.attention                        | BertAttention              | 2 M   \n",
      "132 | model.base_model.encoder.layer.7.attention.self                   | BertSelfAttention          | 1 M   \n",
      "133 | model.base_model.encoder.layer.7.attention.self.query             | Linear                     | 590 K \n",
      "134 | model.base_model.encoder.layer.7.attention.self.key               | Linear                     | 590 K \n",
      "135 | model.base_model.encoder.layer.7.attention.self.value             | Linear                     | 590 K \n",
      "136 | model.base_model.encoder.layer.7.attention.self.dropout           | Dropout                    | 0     \n",
      "137 | model.base_model.encoder.layer.7.attention.output                 | BertSelfOutput             | 592 K \n",
      "138 | model.base_model.encoder.layer.7.attention.output.dense           | Linear                     | 590 K \n",
      "139 | model.base_model.encoder.layer.7.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "140 | model.base_model.encoder.layer.7.attention.output.dropout         | Dropout                    | 0     \n",
      "141 | model.base_model.encoder.layer.7.intermediate                     | BertIntermediate           | 2 M   \n",
      "142 | model.base_model.encoder.layer.7.intermediate.dense               | Linear                     | 2 M   \n",
      "143 | model.base_model.encoder.layer.7.output                           | BertOutput                 | 2 M   \n",
      "144 | model.base_model.encoder.layer.7.output.dense                     | Linear                     | 2 M   \n",
      "145 | model.base_model.encoder.layer.7.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "146 | model.base_model.encoder.layer.7.output.dropout                   | Dropout                    | 0     \n",
      "147 | model.base_model.encoder.layer.8                                  | BertLayer                  | 7 M   \n",
      "148 | model.base_model.encoder.layer.8.attention                        | BertAttention              | 2 M   \n",
      "149 | model.base_model.encoder.layer.8.attention.self                   | BertSelfAttention          | 1 M   \n",
      "150 | model.base_model.encoder.layer.8.attention.self.query             | Linear                     | 590 K \n",
      "151 | model.base_model.encoder.layer.8.attention.self.key               | Linear                     | 590 K \n",
      "152 | model.base_model.encoder.layer.8.attention.self.value             | Linear                     | 590 K \n",
      "153 | model.base_model.encoder.layer.8.attention.self.dropout           | Dropout                    | 0     \n",
      "154 | model.base_model.encoder.layer.8.attention.output                 | BertSelfOutput             | 592 K \n",
      "155 | model.base_model.encoder.layer.8.attention.output.dense           | Linear                     | 590 K \n",
      "156 | model.base_model.encoder.layer.8.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "157 | model.base_model.encoder.layer.8.attention.output.dropout         | Dropout                    | 0     \n",
      "158 | model.base_model.encoder.layer.8.intermediate                     | BertIntermediate           | 2 M   \n",
      "159 | model.base_model.encoder.layer.8.intermediate.dense               | Linear                     | 2 M   \n",
      "160 | model.base_model.encoder.layer.8.output                           | BertOutput                 | 2 M   \n",
      "161 | model.base_model.encoder.layer.8.output.dense                     | Linear                     | 2 M   \n",
      "162 | model.base_model.encoder.layer.8.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "163 | model.base_model.encoder.layer.8.output.dropout                   | Dropout                    | 0     \n",
      "164 | model.base_model.encoder.layer.9                                  | BertLayer                  | 7 M   \n",
      "165 | model.base_model.encoder.layer.9.attention                        | BertAttention              | 2 M   \n",
      "166 | model.base_model.encoder.layer.9.attention.self                   | BertSelfAttention          | 1 M   \n",
      "167 | model.base_model.encoder.layer.9.attention.self.query             | Linear                     | 590 K \n",
      "168 | model.base_model.encoder.layer.9.attention.self.key               | Linear                     | 590 K \n",
      "169 | model.base_model.encoder.layer.9.attention.self.value             | Linear                     | 590 K \n",
      "170 | model.base_model.encoder.layer.9.attention.self.dropout           | Dropout                    | 0     \n",
      "171 | model.base_model.encoder.layer.9.attention.output                 | BertSelfOutput             | 592 K \n",
      "172 | model.base_model.encoder.layer.9.attention.output.dense           | Linear                     | 590 K \n",
      "173 | model.base_model.encoder.layer.9.attention.output.LayerNorm       | LayerNorm                  | 1 K   \n",
      "174 | model.base_model.encoder.layer.9.attention.output.dropout         | Dropout                    | 0     \n",
      "175 | model.base_model.encoder.layer.9.intermediate                     | BertIntermediate           | 2 M   \n",
      "176 | model.base_model.encoder.layer.9.intermediate.dense               | Linear                     | 2 M   \n",
      "177 | model.base_model.encoder.layer.9.output                           | BertOutput                 | 2 M   \n",
      "178 | model.base_model.encoder.layer.9.output.dense                     | Linear                     | 2 M   \n",
      "179 | model.base_model.encoder.layer.9.output.LayerNorm                 | LayerNorm                  | 1 K   \n",
      "180 | model.base_model.encoder.layer.9.output.dropout                   | Dropout                    | 0     \n",
      "181 | model.base_model.encoder.layer.10                                 | BertLayer                  | 7 M   \n",
      "182 | model.base_model.encoder.layer.10.attention                       | BertAttention              | 2 M   \n",
      "183 | model.base_model.encoder.layer.10.attention.self                  | BertSelfAttention          | 1 M   \n",
      "184 | model.base_model.encoder.layer.10.attention.self.query            | Linear                     | 590 K \n",
      "185 | model.base_model.encoder.layer.10.attention.self.key              | Linear                     | 590 K \n",
      "186 | model.base_model.encoder.layer.10.attention.self.value            | Linear                     | 590 K \n",
      "187 | model.base_model.encoder.layer.10.attention.self.dropout          | Dropout                    | 0     \n",
      "188 | model.base_model.encoder.layer.10.attention.output                | BertSelfOutput             | 592 K \n",
      "189 | model.base_model.encoder.layer.10.attention.output.dense          | Linear                     | 590 K \n",
      "190 | model.base_model.encoder.layer.10.attention.output.LayerNorm      | LayerNorm                  | 1 K   \n",
      "191 | model.base_model.encoder.layer.10.attention.output.dropout        | Dropout                    | 0     \n",
      "192 | model.base_model.encoder.layer.10.intermediate                    | BertIntermediate           | 2 M   \n",
      "193 | model.base_model.encoder.layer.10.intermediate.dense              | Linear                     | 2 M   \n",
      "194 | model.base_model.encoder.layer.10.output                          | BertOutput                 | 2 M   \n",
      "195 | model.base_model.encoder.layer.10.output.dense                    | Linear                     | 2 M   \n",
      "196 | model.base_model.encoder.layer.10.output.LayerNorm                | LayerNorm                  | 1 K   \n",
      "197 | model.base_model.encoder.layer.10.output.dropout                  | Dropout                    | 0     \n",
      "198 | model.base_model.encoder.layer.11                                 | BertLayer                  | 7 M   \n",
      "199 | model.base_model.encoder.layer.11.attention                       | BertAttention              | 2 M   \n",
      "200 | model.base_model.encoder.layer.11.attention.self                  | BertSelfAttention          | 1 M   \n",
      "201 | model.base_model.encoder.layer.11.attention.self.query            | Linear                     | 590 K \n",
      "202 | model.base_model.encoder.layer.11.attention.self.key              | Linear                     | 590 K \n",
      "203 | model.base_model.encoder.layer.11.attention.self.value            | Linear                     | 590 K \n",
      "204 | model.base_model.encoder.layer.11.attention.self.dropout          | Dropout                    | 0     \n",
      "205 | model.base_model.encoder.layer.11.attention.output                | BertSelfOutput             | 592 K \n",
      "206 | model.base_model.encoder.layer.11.attention.output.dense          | Linear                     | 590 K \n",
      "207 | model.base_model.encoder.layer.11.attention.output.LayerNorm      | LayerNorm                  | 1 K   \n",
      "208 | model.base_model.encoder.layer.11.attention.output.dropout        | Dropout                    | 0     \n",
      "209 | model.base_model.encoder.layer.11.intermediate                    | BertIntermediate           | 2 M   \n",
      "210 | model.base_model.encoder.layer.11.intermediate.dense              | Linear                     | 2 M   \n",
      "211 | model.base_model.encoder.layer.11.output                          | BertOutput                 | 2 M   \n",
      "212 | model.base_model.encoder.layer.11.output.dense                    | Linear                     | 2 M   \n",
      "213 | model.base_model.encoder.layer.11.output.LayerNorm                | LayerNorm                  | 1 K   \n",
      "214 | model.base_model.encoder.layer.11.output.dropout                  | Dropout                    | 0     \n",
      "215 | model.base_model.pooler                                           | BertPooler                 | 590 K \n",
      "216 | model.base_model.pooler.dense                                     | Linear                     | 590 K \n",
      "217 | model.base_model.pooler.activation                                | Tanh                       | 0     \n",
      "218 | model.classifier                                                  | Sequential                 | 592 K \n",
      "219 | model.classifier.0                                                | Dropout                    | 0     \n",
      "220 | model.classifier.1                                                | Linear                     | 590 K \n",
      "221 | model.classifier.2                                                | Mish                       | 0     \n",
      "222 | model.classifier.3                                                | Dropout                    | 0     \n",
      "223 | model.classifier.4                                                | Linear                     | 1 K   \n",
      "224 | loss                                                              | CrossEntropyLoss           | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  96%|█████████▌| 130/136 [00:17<00:00,  7.54it/s, loss=0.059, train_loss=0.00898, v_num=123]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train (using cuda)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "                     early_stop_callback=early_stop_callback)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# module.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n",
      "________________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        weak     0.7353    0.6944    0.7143        36\n",
      "      strong     0.7027    0.7429    0.7222        35\n",
      "\n",
      "    accuracy                         0.7183        71\n",
      "   macro avg     0.7190    0.7187    0.7183        71\n",
      "weighted avg     0.7192    0.7183    0.7182        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval().cuda()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        X,y = batch_\n",
    "        input_ids = X[0]\n",
    "        attention_mask = X[1]\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        y_pred = torch.argmax(module(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(true_y, pred_y, labels=range(len(labels)))\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cm, annot=True, cmap='Greens', fmt='g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
