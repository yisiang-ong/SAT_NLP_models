{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from typing import List\n",
    "import logging\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "from argparse import Namespace\n",
    "from packaging import version\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is partly adapted from the link below:\n",
    "# https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"empathy_dataset/my_train.txt\"\n",
    "test_path = \"empathy_dataset/my_test.txt\"\n",
    "val_path = \"empathy_dataset/my_val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weak': 0, 'strong': 1}\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary associating each string label to an integer value\n",
    "\n",
    "labels = [\"weak\", \"strong\"]\n",
    "label2int = dict(zip(labels, list(range(len(labels)))))\n",
    "print(label2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpathyDataset(Dataset):\n",
    "  def __init__(\n",
    "    self,\n",
    "    path,\n",
    "    tokenizer: DistilBertTokenizer,\n",
    "    max_token_len: int = 100\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data_column = \"text\"\n",
    "    self.class_column = \"class\"\n",
    "    self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                            engine=\"python\")\n",
    "    \n",
    "    self.max_token_len = max_token_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "    text = data_row.text\n",
    "    labels = label2int[data_row[\"class\"]]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return (encoding[\"input_ids\"].flatten(),encoding[\"attention_mask\"].flatten()), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 101, 1045, 2113, 2009, 2089, 4025, 3697, 2021, 3046, 2085, 2023, 1063,\n",
       "          8778, 1065, 1012, 2903, 1999, 1996, 2047, 2217, 2017, 2024, 2055, 2000,\n",
       "          7523, 1010, 2043, 2017, 2514, 2157, 1010, 2718, 3613, 2000, 2693, 2006,\n",
       "          1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0])),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EmpathyDataset(\n",
    "    train_path,\n",
    "    tokenizer,\n",
    "    max_token_len=100\n",
    ")\n",
    "sample_item = train_dataset[5]\n",
    "sample_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom classifcation model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Mish activation function \n",
    "#(from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py)\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "  \n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an EmpathyClassificationModel class to do the actual fine-tuning\n",
    "\n",
    "class EmpathyClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "        \n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, *args):\n",
    "\n",
    "        hidden_states = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PyTorch Lightning for training.\n",
    "#we use PyTorch Lighning for training. Lightning methods are defined here\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmpathyClassificationModel(DistilBertModel.from_pretrained('distilbert-base-uncased'), len(labels)) #was \"distilroberta-base\"\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.max_token_length = 128\n",
    "        self.loss = nn.CrossEntropyLoss() #cross entropy loss since this is multi-class classification\n",
    "        # self.save_hyperparameters(hparams)\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        # print(self.forward(input_ids=X[0], attention_mask = X[1]))\n",
    "        loss = self.loss(self.forward(input_ids=X[0], attention_mask = X[1]), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        \n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmpathyDataset(ds_path, self.tokenizer, self.max_token_length),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr) #we use AdamW as this usually performs well\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n",
    "   \n",
    "    # def save_model(self, version):\n",
    "    #     torch.save(self.model.state_dict(), f'empathy_model/BERT_empathy_1ft_{version}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=20,\n",
    "    warmup_steps=100,\n",
    "    epochs=20,\n",
    "    lr=2E-06,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)\n",
    "#rubbish collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # monitor validation loss\n",
    "    min_delta=0.001, #to very small change in the monitored quantity to qualify as an improvement\n",
    "    patience=20, # used to check number of time with no improvement after which training will be stopped\n",
    "    verbose=False, \n",
    "    mode=\"min\" #sed while training will stopped when the quantity monitor has stopped decreasing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:23: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:lightning:\n",
      "   | Name                                                   | Type                       | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0  | model                                                  | EmpathyClassificationModel | 66 M  \n",
      "1  | model.base_model                                       | DistilBertModel            | 66 M  \n",
      "2  | model.base_model.embeddings                            | Embeddings                 | 23 M  \n",
      "3  | model.base_model.embeddings.word_embeddings            | Embedding                  | 23 M  \n",
      "4  | model.base_model.embeddings.position_embeddings        | Embedding                  | 393 K \n",
      "5  | model.base_model.embeddings.LayerNorm                  | LayerNorm                  | 1 K   \n",
      "6  | model.base_model.embeddings.dropout                    | Dropout                    | 0     \n",
      "7  | model.base_model.transformer                           | Transformer                | 42 M  \n",
      "8  | model.base_model.transformer.layer                     | ModuleList                 | 42 M  \n",
      "9  | model.base_model.transformer.layer.0                   | TransformerBlock           | 7 M   \n",
      "10 | model.base_model.transformer.layer.0.attention         | MultiHeadSelfAttention     | 2 M   \n",
      "11 | model.base_model.transformer.layer.0.attention.dropout | Dropout                    | 0     \n",
      "12 | model.base_model.transformer.layer.0.attention.q_lin   | Linear                     | 590 K \n",
      "13 | model.base_model.transformer.layer.0.attention.k_lin   | Linear                     | 590 K \n",
      "14 | model.base_model.transformer.layer.0.attention.v_lin   | Linear                     | 590 K \n",
      "15 | model.base_model.transformer.layer.0.attention.out_lin | Linear                     | 590 K \n",
      "16 | model.base_model.transformer.layer.0.sa_layer_norm     | LayerNorm                  | 1 K   \n",
      "17 | model.base_model.transformer.layer.0.ffn               | FFN                        | 4 M   \n",
      "18 | model.base_model.transformer.layer.0.ffn.dropout       | Dropout                    | 0     \n",
      "19 | model.base_model.transformer.layer.0.ffn.lin1          | Linear                     | 2 M   \n",
      "20 | model.base_model.transformer.layer.0.ffn.lin2          | Linear                     | 2 M   \n",
      "21 | model.base_model.transformer.layer.0.ffn.activation    | GELUActivation             | 0     \n",
      "22 | model.base_model.transformer.layer.0.output_layer_norm | LayerNorm                  | 1 K   \n",
      "23 | model.base_model.transformer.layer.1                   | TransformerBlock           | 7 M   \n",
      "24 | model.base_model.transformer.layer.1.attention         | MultiHeadSelfAttention     | 2 M   \n",
      "25 | model.base_model.transformer.layer.1.attention.dropout | Dropout                    | 0     \n",
      "26 | model.base_model.transformer.layer.1.attention.q_lin   | Linear                     | 590 K \n",
      "27 | model.base_model.transformer.layer.1.attention.k_lin   | Linear                     | 590 K \n",
      "28 | model.base_model.transformer.layer.1.attention.v_lin   | Linear                     | 590 K \n",
      "29 | model.base_model.transformer.layer.1.attention.out_lin | Linear                     | 590 K \n",
      "30 | model.base_model.transformer.layer.1.sa_layer_norm     | LayerNorm                  | 1 K   \n",
      "31 | model.base_model.transformer.layer.1.ffn               | FFN                        | 4 M   \n",
      "32 | model.base_model.transformer.layer.1.ffn.dropout       | Dropout                    | 0     \n",
      "33 | model.base_model.transformer.layer.1.ffn.lin1          | Linear                     | 2 M   \n",
      "34 | model.base_model.transformer.layer.1.ffn.lin2          | Linear                     | 2 M   \n",
      "35 | model.base_model.transformer.layer.1.output_layer_norm | LayerNorm                  | 1 K   \n",
      "36 | model.base_model.transformer.layer.2                   | TransformerBlock           | 7 M   \n",
      "37 | model.base_model.transformer.layer.2.attention         | MultiHeadSelfAttention     | 2 M   \n",
      "38 | model.base_model.transformer.layer.2.attention.dropout | Dropout                    | 0     \n",
      "39 | model.base_model.transformer.layer.2.attention.q_lin   | Linear                     | 590 K \n",
      "40 | model.base_model.transformer.layer.2.attention.k_lin   | Linear                     | 590 K \n",
      "41 | model.base_model.transformer.layer.2.attention.v_lin   | Linear                     | 590 K \n",
      "42 | model.base_model.transformer.layer.2.attention.out_lin | Linear                     | 590 K \n",
      "43 | model.base_model.transformer.layer.2.sa_layer_norm     | LayerNorm                  | 1 K   \n",
      "44 | model.base_model.transformer.layer.2.ffn               | FFN                        | 4 M   \n",
      "45 | model.base_model.transformer.layer.2.ffn.dropout       | Dropout                    | 0     \n",
      "46 | model.base_model.transformer.layer.2.ffn.lin1          | Linear                     | 2 M   \n",
      "47 | model.base_model.transformer.layer.2.ffn.lin2          | Linear                     | 2 M   \n",
      "48 | model.base_model.transformer.layer.2.output_layer_norm | LayerNorm                  | 1 K   \n",
      "49 | model.base_model.transformer.layer.3                   | TransformerBlock           | 7 M   \n",
      "50 | model.base_model.transformer.layer.3.attention         | MultiHeadSelfAttention     | 2 M   \n",
      "51 | model.base_model.transformer.layer.3.attention.dropout | Dropout                    | 0     \n",
      "52 | model.base_model.transformer.layer.3.attention.q_lin   | Linear                     | 590 K \n",
      "53 | model.base_model.transformer.layer.3.attention.k_lin   | Linear                     | 590 K \n",
      "54 | model.base_model.transformer.layer.3.attention.v_lin   | Linear                     | 590 K \n",
      "55 | model.base_model.transformer.layer.3.attention.out_lin | Linear                     | 590 K \n",
      "56 | model.base_model.transformer.layer.3.sa_layer_norm     | LayerNorm                  | 1 K   \n",
      "57 | model.base_model.transformer.layer.3.ffn               | FFN                        | 4 M   \n",
      "58 | model.base_model.transformer.layer.3.ffn.dropout       | Dropout                    | 0     \n",
      "59 | model.base_model.transformer.layer.3.ffn.lin1          | Linear                     | 2 M   \n",
      "60 | model.base_model.transformer.layer.3.ffn.lin2          | Linear                     | 2 M   \n",
      "61 | model.base_model.transformer.layer.3.output_layer_norm | LayerNorm                  | 1 K   \n",
      "62 | model.base_model.transformer.layer.4                   | TransformerBlock           | 7 M   \n",
      "63 | model.base_model.transformer.layer.4.attention         | MultiHeadSelfAttention     | 2 M   \n",
      "64 | model.base_model.transformer.layer.4.attention.dropout | Dropout                    | 0     \n",
      "65 | model.base_model.transformer.layer.4.attention.q_lin   | Linear                     | 590 K \n",
      "66 | model.base_model.transformer.layer.4.attention.k_lin   | Linear                     | 590 K \n",
      "67 | model.base_model.transformer.layer.4.attention.v_lin   | Linear                     | 590 K \n",
      "68 | model.base_model.transformer.layer.4.attention.out_lin | Linear                     | 590 K \n",
      "69 | model.base_model.transformer.layer.4.sa_layer_norm     | LayerNorm                  | 1 K   \n",
      "70 | model.base_model.transformer.layer.4.ffn               | FFN                        | 4 M   \n",
      "71 | model.base_model.transformer.layer.4.ffn.dropout       | Dropout                    | 0     \n",
      "72 | model.base_model.transformer.layer.4.ffn.lin1          | Linear                     | 2 M   \n",
      "73 | model.base_model.transformer.layer.4.ffn.lin2          | Linear                     | 2 M   \n",
      "74 | model.base_model.transformer.layer.4.output_layer_norm | LayerNorm                  | 1 K   \n",
      "75 | model.base_model.transformer.layer.5                   | TransformerBlock           | 7 M   \n",
      "76 | model.base_model.transformer.layer.5.attention         | MultiHeadSelfAttention     | 2 M   \n",
      "77 | model.base_model.transformer.layer.5.attention.dropout | Dropout                    | 0     \n",
      "78 | model.base_model.transformer.layer.5.attention.q_lin   | Linear                     | 590 K \n",
      "79 | model.base_model.transformer.layer.5.attention.k_lin   | Linear                     | 590 K \n",
      "80 | model.base_model.transformer.layer.5.attention.v_lin   | Linear                     | 590 K \n",
      "81 | model.base_model.transformer.layer.5.attention.out_lin | Linear                     | 590 K \n",
      "82 | model.base_model.transformer.layer.5.sa_layer_norm     | LayerNorm                  | 1 K   \n",
      "83 | model.base_model.transformer.layer.5.ffn               | FFN                        | 4 M   \n",
      "84 | model.base_model.transformer.layer.5.ffn.dropout       | Dropout                    | 0     \n",
      "85 | model.base_model.transformer.layer.5.ffn.lin1          | Linear                     | 2 M   \n",
      "86 | model.base_model.transformer.layer.5.ffn.lin2          | Linear                     | 2 M   \n",
      "87 | model.base_model.transformer.layer.5.output_layer_norm | LayerNorm                  | 1 K   \n",
      "88 | model.classifier                                       | Sequential                 | 592 K \n",
      "89 | model.classifier.0                                     | Dropout                    | 0     \n",
      "90 | model.classifier.1                                     | Linear                     | 590 K \n",
      "91 | model.classifier.2                                     | Mish                       | 0     \n",
      "92 | model.classifier.3                                     | Dropout                    | 0     \n",
      "93 | model.classifier.4                                     | Linear                     | 1 K   \n",
      "94 | loss                                                   | CrossEntropyLoss           | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  87%|████████▋ | 60/69 [00:22<00:03,  2.67it/s, loss=0.453, train_loss=0.368, v_num=131]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train (using cuda)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "                     early_stop_callback=early_stop_callback)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n",
      "________________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        weak     0.6579    0.6944    0.6757        36\n",
      "      strong     0.6667    0.6286    0.6471        35\n",
      "\n",
      "    accuracy                         0.6620        71\n",
      "   macro avg     0.6623    0.6615    0.6614        71\n",
      "weighted avg     0.6622    0.6620    0.6616        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval().cuda()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        X,y = batch_\n",
    "        input_ids = X[0]\n",
    "        attention_mask = X[1]\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        y_pred = torch.argmax(module(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(true_y, pred_y, labels=range(len(labels)))\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cm, annot=True, cmap='Greens', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# module.save_model(version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model test\n",
    "# hparams = Namespace(\n",
    "#     train_path=train_path,\n",
    "#     val_path=val_path,\n",
    "#     test_path=test_path,\n",
    "#     batch_size=10,\n",
    "#     warmup_steps=100,\n",
    "#     epochs=20,\n",
    "#     lr=2.5E-05,\n",
    "#     accumulate_grad_batches=1\n",
    "# )\n",
    "# device = torch.device('cuda:0')\n",
    "# model = TrainingModule(hparams)\n",
    "# model.model.load_state_dict(torch.load('empathy_model\\BERT_empathy_1ft_1.pt'), strict=False)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing for retrieving model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# with torch.no_grad():\n",
    "#     progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "#     model.eval().cuda()\n",
    "#     true_y, pred_y = [], []\n",
    "#     for i, batch_ in enumerate(model.test_dataloader()):\n",
    "#         X,y = batch_\n",
    "#         input_ids = X[0]\n",
    "#         attention_mask = X[1]\n",
    "#         print(progress[i % len(progress)], end=\"\\r\")\n",
    "#         y_pred = torch.argmax(model(input_ids.to(device), attention_mask.to(device)), dim=1)\n",
    "#         true_y.extend(y.cpu())\n",
    "#         pred_y.extend(y_pred.cpu())\n",
    "# print(\"\\n\" + \"_\" * 80)\n",
    "# print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
